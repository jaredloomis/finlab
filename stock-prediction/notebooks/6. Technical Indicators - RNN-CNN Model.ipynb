{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3112be5d",
   "metadata": {},
   "source": [
    "# Technical Indicators - NN Models\n",
    "\n",
    "We've tried training most of the basic models provided by scikit on technical data from individual assets - with a good amount of success. The most effective model was the Random Forest model, which does not recognize any temporal relationship between any two feature sets (ie. if you provide samples to the model in reverse you get the same result).\n",
    "\n",
    "RNNs (and to some degree CNNs) are relatively unique in the landscape of machine learning models in that temporal relationships can be encoded and learned fairly easily. This is perfect for our use case of purely market temporal data.\n",
    "\n",
    "Given the wide array of neural network architectures in use today, it makes sense to explore the results we can get from a variety of NN architectures. For now we will experiment without changing the format or content of the input data.\n",
    "\n",
    "Input will be the historical technical indicators data for a single asset **One model should be trained for each asset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b0368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5630, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/ta/trend.py:769: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/ta/trend.py:774: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    }
   ],
   "source": [
    "# Auto reload local files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Make files in src/ available to notebook\n",
    "import sys\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.insert(0, 'src')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from technical_signals import TechnicalSignals, percent_change\n",
    "import datastore as ds\n",
    "\n",
    "class TechnicalIndicatorsDataset(Dataset):\n",
    "    def __init__(self, ticker, predict_window, transform=None, target_transform=None):\n",
    "        data = ds.get_daily_candlesticks([ticker], \"2000-01-01\", \"2040-06-06\")[ticker]\n",
    "        indicators = TechnicalSignals(data, predict_window=predict_window)\n",
    "        self.X, self.y, self.date = indicators.toXy()\n",
    "        print(self.X.shape)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[:, idx]\n",
    "        label = self.y[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "train_dataset = TechnicalIndicatorsDataset('AAPL', predict_window=7)\n",
    "n_features = train_dataset.X.shape[1]\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#test_dataloader = torch.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4a9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_579884/4074662948.py\", line 35, in __getitem__\n    sample = self.X[:, idx]\nIndexError: index 1856 is out of bounds for axis 1 with size 38\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Pass input to a 1D convolutional layer with a kernel size of 3, apply to activation function.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv1d(n_features, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#nn.Softmax()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Test, print basic info about model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     44\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel input shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1250\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jared/.local/share/virtualenvs/notebooks-AmY4tn1L/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_579884/4074662948.py\", line 35, in __getitem__\n    sample = self.X[:, idx]\nIndexError: index 1856 is out of bounds for axis 1 with size 38\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#n_features = 20\n",
    "n_outputs = 1\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # Pass input to a 1D convolutional layer with a kernel size of 3, apply to activation function.\n",
    "    nn.Conv1d(n_features, 32, 3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel.\n",
    "    nn.Conv1d(32, 32, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel. (same as previous layer)\n",
    "    nn.Conv1d(32, 32, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Flatten the convolutions. Input shape: (a, b, c), Output shape: (a, b*c)\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    #nn.Dropout(0.5),\n",
    "    # ?\n",
    "    # XXX: The first number needs to be updated each time the input shapes change. We could instead\n",
    "    #      Create a class-based Module, and do a single pass through the conv portion of the network\n",
    "    #      in order to determine the actual size.\n",
    "    #      (This technique is shown in https://www.youtube.com/watch?v=1gQR24B3ISE&list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh&index=7).\n",
    "    #      For now, we can update this value as needed by commenting out all layers after Flatten(), then running the code\n",
    "    #      below and inspecting the output shape. The x[1] value should be the first arg in the following line.\n",
    "    #nn.Linear(1696, 512),  # ~= nn.LazyLinear(512)\n",
    "    nn.LazyLinear(512),\n",
    "\n",
    "    # Flatten the linear layer into the required number of outputs\n",
    "    nn.Linear(512, n_outputs),\n",
    "    #nn.Softmax()\n",
    ")\n",
    "\n",
    "# Test, print basic info about model\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    features, labels = data\n",
    "    print(\"Model input shape:\", features.shape)\n",
    "    out = net(features.float())\n",
    "    print(\"Model output shape:\", out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735c0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
