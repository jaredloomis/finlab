{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed3e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Suppress ta warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Auto reload local files\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Make files in src/ available to notebook\n",
    "import sys\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Read SPY csv, define config\n",
    "watchlist = list(pd.read_csv('../data/watchlist.csv', header=0)['symbol'])\n",
    "spy_constituents = list(pd.read_csv('../../data/spy_constituents.csv', header=0)['Symbol'])\n",
    "random.shuffle(spy_constituents)\n",
    "\n",
    "# Current tickers we have in database\n",
    "tickers = [\"AAL\",\"AAP\",\"AAPL\",\"ACN\",\"ADP\",\"ADX\",\"AEE\",\"AEP\",\"AFL\",\"AIV\",\"AJG\",\"AKAM\",\"ALGN\",\"AMAT\",\"AMD\",\"AMN\",\"AMP\",\"AMT\",\"ANSS\",\"AON\",\"AOS\",\"ARA\",\"ARE\",\"ATGE\",\"ATUS\",\"AVGO\",\"BA\",\"BBWI\",\"BCEI\",\"BK\",\"BKN\",\"BKNG\",\"BKR\",\"BLK\",\"BLL\",\"BRK.B\",\"BRO\",\"BTC-USD\",\"BYM\",\"CAAP\",\"CAT\",\"CBRE\",\"CBT\",\"CELG~\",\"CF\",\"CHD\",\"CHRW\",\"CIA\",\"CL\",\"CMCSA\",\"CMI\",\"CMS\",\"CNC\",\"COF\",\"COP\",\"COST\",\"CPRT\",\"CRL\",\"CSX\",\"CTLT\",\"CTRA\",\"CTSH\",\"CUZ\",\"CYD\",\"DD\",\"DEI\",\"DHR\",\"DIS\",\"DOV\",\"DOW\",\"DPW\",\"DPZ\",\"DRE\",\"DRI\",\"DRQ\",\"DVA\",\"DXC\",\"DXCM\",\"ECL\",\"EFX\",\"EL\",\"ELC\",\"EMR\",\"ES\",\"EVA\",\"EXPD\",\"EXR\",\"FANG\",\"FBHS\",\"FE\",\"FLT\",\"FOX\",\"FPAC\",\"GE\",\"GEF\",\"GLW\",\"GM\",\"GNRC\",\"GOOG\",\"GPC\",\"GPM\",\"GPS\",\"GWW\",\"HAL\",\"HD\",\"HESM\",\"HOLX\",\"HOV\",\"HPE\",\"HPQ\",\"HQL\",\"HRI\",\"HRL\",\"HUBB\",\"HUYA\",\"ICE\",\"INFO\",\"INTC\",\"IPG\",\"IQV\",\"IRM\",\"ISD\",\"ISRG\",\"J\",\"JCI\",\"JDD\",\"JNJ\",\"KEY\",\"KO\",\"KRO\",\"LEN\",\"LH\",\"LHX\",\"LNC\",\"LRCX\",\"LTC-USD\",\"LYV\",\"MA\",\"MAXR\",\"MC\",\"MCR\",\"MDLZ\",\"MEG\",\"MLM\",\"MMI\",\"MMM\",\"MO\",\"MPC\",\"MPWR\",\"MRK\",\"MRO\",\"MSFT\",\"MSGS\",\"MTB\",\"NAVB\",\"NEM\",\"NFJ\",\"NLSN\",\"NML\",\"NNA\",\"NOC\",\"NRG\",\"NTP\",\"NVDA\",\"NWS\",\"NXRT\",\"OKE\",\"ORA\",\"OTIS\",\"PAYC\",\"PCAR\",\"PEAK\",\"PENN\",\"PHM\",\"PHX\",\"PLYM\",\"PNR\",\"PRU\",\"PWR\",\"PYS\",\"QCOM\",\"RCA\",\"REGN\",\"RF\",\"RJF\",\"ROST\",\"RSG\",\"SCHW\",\"SEE\",\"SLCA\",\"SMLP\",\"SOR\",\"SPH\",\"SRL\",\"STE\",\"SWK\",\"SWKS\",\"T\",\"TAP\",\"TDS\",\"TDY\",\"TECH\",\"TEN\",\"THO\",\"TMO\",\"TNK\",\"TPR\",\"TRMB\",\"TSCO\",\"TSLA\",\"TT\",\"TWTR\",\"TXN\",\"TXT\",\"TYL\",\"UDR\",\"UNF\",\"UNH\",\"UPS\",\"USB\",\"V\",\"VCIF\",\"VFC\",\"VMC\",\"VNO\",\"VRSK\",\"VTI\",\"VTR\",\"WEC\",\"WELL\",\"WFC\",\"WIT\",\"WMB\",\"WU\",\"XOM\",\"XPO\",\"YUM\",\"YUMC\",\"ZBH\",\"ZBRA\",\"ZION\",\"ZTS\"]\n",
    "start_date = pd.to_datetime(\"2019-01-01\")\n",
    "end_date = datetime.now() + timedelta(hours=1)\n",
    "predict_window = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23853eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from db\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datastore as ds\n",
    "from technical_signals import TechnicalSignalSet\n",
    "\n",
    "#ds.download_candles(tickers, start_date, end_date)\n",
    "def get_train_test(tickers, start_date, end_date):\n",
    "    candlesticks = ds.get_candles(tickers, start_date, end_date, interval='5min')\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            technical_sigs = TechnicalSignalSet(candlesticks[ticker], predict_window)\n",
    "            X, y, Xy_date = technical_sigs.to_xy()\n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "        except Exception as ex:\n",
    "            print(f\"Exception on {ticker}:\")\n",
    "            print(ex)\n",
    "\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b468e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541efd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "\n",
    "def round_batch_size(sample_count, approximately, leeway=None):\n",
    "    \"\"\"\n",
    "    Round batch size to a more suitable value. This helps to avoid a\n",
    "    problem where the final batch has a lot of samples, but not enough for\n",
    "    a full batch, leading to many samples being thrown out.\n",
    "\n",
    "    approximately: int, leeway: int\n",
    "      decide on a chunk size around a number, with specified leeway\n",
    "      (leeway defaults to `approximately // 10`).\n",
    "    \"\"\"\n",
    "    if leeway is None:\n",
    "        leeway = approximately // 10\n",
    "    \n",
    "    # Get the number of leftover samples if we use the suggested batch size\n",
    "    best_leftover = sample_count - np.floor(sample_count / approximately) * approximately\n",
    "\n",
    "    # Brute-force search for the value that yeilds the fewest leftovers\n",
    "    # within the given leeway range.\n",
    "    best_chunk_count = approximately\n",
    "    for offset in range(-leeway, leeway):\n",
    "        chunk_size = approximately + offset\n",
    "        leftover = sample_count - np.floor(sample_count / chunk_size) * chunk_size\n",
    "        if leftover < best_leftover:\n",
    "            best_leftover = leftover\n",
    "            best_chunk_count = chunk_size\n",
    "    return best_chunk_count\n",
    "            \n",
    "def get_train_val_dataloaders(tickers, start, end):\n",
    "    X_train, X_test, y_train, y_test = get_train_test(tickers, start, end)\n",
    "    \n",
    "    batch_size = round_batch_size(X_train.shape[0], 8096, leeway=200)\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    # Convert X, y to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train.reshape(y_train.shape[0], 1)).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test.reshape(y_test.shape[0], 1)).float()\n",
    "\n",
    "    print(X_train_tensor.shape)\n",
    "    print('Batch size:', batch_size)\n",
    "\n",
    "    # Generators\n",
    "    training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    dataloader_train = DataLoader(training_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    validation_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    dataloader_val = DataLoader(validation_set, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    del X_train_tensor\n",
    "    del X_test_tensor\n",
    "    del y_train_tensor\n",
    "    del y_test_tensor\n",
    "    \n",
    "    return dataloader_train, dataloader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ec55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train(net, criterion, optimizer, dataloader_train, dataloader_val, epochs=100, device='cuda'):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training\n",
    "        net.train()\n",
    "        for local_batch, local_labels in dataloader_train:\n",
    "            #if local_batch.shape[0] != batch_size:\n",
    "            #    print(f\"Wrong train batch size. Skipping batch.\\nThrowing away {local_batch.shape[0]} samples.\")\n",
    "            #    continue\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Forward pass: Compute predicted y by passing x to the model \n",
    "            y_pred = net(local_batch)\n",
    "            # Compute and print loss \n",
    "            loss = criterion(y_pred, local_labels)\n",
    "            # Zero gradients, perform a backward pass, update the weights. \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            # Update loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        valid_loss = 0.0\n",
    "        for data, labels in dataloader_val:\n",
    "            #if data.shape[0] != batch_size:\n",
    "            #    continue\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            target = net(data)\n",
    "            loss = criterion(target,labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(dataloader_train)} \\t\\t Validation Loss: {valid_loss / len(dataloader_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c024be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 1\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, n_outputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d068c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TSLA' 'HPQ' 'ARE' 'AEP' 'AAPL' 'ADP' 'OKE' 'V' 'PRU' 'RJF' 'MPC' 'EFX'\n",
      " 'BKN' 'EXR' 'UPS' 'INTC' 'BKNG' 'JCI' 'WIT' 'AFL' 'AMT' 'AVGO' 'ANSS'\n",
      " 'NLSN' 'GM' 'LHX' 'HESM' 'DIS' 'JDD' 'FBHS' 'ISD' 'CRL' 'BRO' 'CHD'\n",
      " 'TRMB']\n",
      "torch.Size([1445406, 58])\n",
      "Batch size: 8030\n",
      "Epoch 1 \t\t Training Loss: 0.9972086120705578 \t\t Validation Loss: 0.9823698163032532\n",
      "Epoch 2 \t\t Training Loss: 0.987365596202197 \t\t Validation Loss: 0.9762635290622711\n",
      "Epoch 3 \t\t Training Loss: 0.9823555291028313 \t\t Validation Loss: 0.9790523052215576\n",
      "Epoch 4 \t\t Training Loss: 0.9829383614313537 \t\t Validation Loss: 0.9712330102920532\n",
      "Epoch 5 \t\t Training Loss: 0.9769775893806752 \t\t Validation Loss: 0.967155373096466\n",
      "Epoch 6 \t\t Training Loss: 0.9747128937784479 \t\t Validation Loss: 0.9688846886157989\n",
      "Epoch 7 \t\t Training Loss: 0.9714990658325385 \t\t Validation Loss: 0.9515449106693268\n",
      "Epoch 8 \t\t Training Loss: 0.9573479989615593 \t\t Validation Loss: 0.9520069777965545\n",
      "Epoch 9 \t\t Training Loss: 0.9501943608015282 \t\t Validation Loss: 0.9377390027046204\n",
      "Epoch 10 \t\t Training Loss: 0.9365432453418964 \t\t Validation Loss: 0.9395506918430329\n",
      "Epoch 11 \t\t Training Loss: 0.9368084066480563 \t\t Validation Loss: 0.9680997550487518\n",
      "Epoch 12 \t\t Training Loss: 0.9355176934221173 \t\t Validation Loss: 0.9302395522594452\n",
      "Epoch 13 \t\t Training Loss: 0.9290444725784808 \t\t Validation Loss: 0.923220157623291\n",
      "Epoch 14 \t\t Training Loss: 0.9221150437112671 \t\t Validation Loss: 0.9377271831035614\n",
      "Epoch 15 \t\t Training Loss: 0.9162452461311171 \t\t Validation Loss: 0.9159424424171447\n",
      "Epoch 16 \t\t Training Loss: 0.9153820733997703 \t\t Validation Loss: 0.9540672183036805\n",
      "Epoch 17 \t\t Training Loss: 0.9101932868773107 \t\t Validation Loss: 0.8992045521736145\n",
      "Epoch 18 \t\t Training Loss: 0.8783117236353416 \t\t Validation Loss: 0.8917819023132324\n",
      "Epoch 19 \t\t Training Loss: 0.8740366292263263 \t\t Validation Loss: 0.8779379963874817\n",
      "Epoch 20 \t\t Training Loss: 0.864433797683505 \t\t Validation Loss: 0.9012747168540954\n",
      "Epoch 21 \t\t Training Loss: 0.864591552407702 \t\t Validation Loss: 0.9054627001285553\n",
      "Epoch 22 \t\t Training Loss: 0.8650061537547665 \t\t Validation Loss: 0.9659593164920807\n",
      "Epoch 23 \t\t Training Loss: 0.9028818304367487 \t\t Validation Loss: 0.8941524982452392\n",
      "Epoch 24 \t\t Training Loss: 0.8572679076405519 \t\t Validation Loss: 0.8582623660564422\n",
      "Epoch 25 \t\t Training Loss: 0.8426598629898788 \t\t Validation Loss: 0.8755908727645874\n",
      "Epoch 26 \t\t Training Loss: 0.8375647410503408 \t\t Validation Loss: 0.851365464925766\n",
      "Epoch 27 \t\t Training Loss: 0.840127347911919 \t\t Validation Loss: 0.9117418944835662\n",
      "Epoch 28 \t\t Training Loss: 0.8889910740088363 \t\t Validation Loss: 0.8591746807098388\n",
      "Epoch 29 \t\t Training Loss: 0.845174269122972 \t\t Validation Loss: 0.8489142894744873\n",
      "Epoch 30 \t\t Training Loss: 0.8370649281127677 \t\t Validation Loss: 0.8626581728458405\n",
      "Epoch 31 \t\t Training Loss: 0.8513895256743247 \t\t Validation Loss: 0.8477710723876953\n",
      "Epoch 32 \t\t Training Loss: 0.8199191992454107 \t\t Validation Loss: 0.8397824347019196\n",
      "Epoch 33 \t\t Training Loss: 0.8240490030517894 \t\t Validation Loss: 0.8311596035957336\n",
      "Epoch 34 \t\t Training Loss: 0.7998488077801236 \t\t Validation Loss: 0.839082396030426\n",
      "Epoch 35 \t\t Training Loss: 0.8140382355089346 \t\t Validation Loss: 0.8397384285926819\n",
      "Epoch 36 \t\t Training Loss: 0.8192948804375875 \t\t Validation Loss: 0.86361945271492\n",
      "Epoch 37 \t\t Training Loss: 0.8225587662412317 \t\t Validation Loss: 0.8242747724056244\n",
      "Epoch 38 \t\t Training Loss: 0.8048992871579544 \t\t Validation Loss: 0.8271929383277893\n",
      "Epoch 39 \t\t Training Loss: 0.786289839619431 \t\t Validation Loss: 0.7987374126911163\n",
      "Epoch 40 \t\t Training Loss: 0.7775736961575502 \t\t Validation Loss: 0.8017107486724854\n",
      "Epoch 41 \t\t Training Loss: 0.770397374478493 \t\t Validation Loss: 0.7921125411987304\n",
      "Epoch 42 \t\t Training Loss: 0.7633388564402227 \t\t Validation Loss: 0.8172052085399628\n",
      "Epoch 43 \t\t Training Loss: 0.7698555641411418 \t\t Validation Loss: 0.7875243902206421\n",
      "Epoch 44 \t\t Training Loss: 0.7639812333149146 \t\t Validation Loss: 0.8043161273002625\n",
      "Epoch 45 \t\t Training Loss: 0.7614985391579939 \t\t Validation Loss: 0.80639568567276\n",
      "Epoch 46 \t\t Training Loss: 0.7575244531447057 \t\t Validation Loss: 0.7889965534210205\n",
      "Epoch 47 \t\t Training Loss: 0.7585816827926847 \t\t Validation Loss: 0.8239948570728302\n",
      "Epoch 48 \t\t Training Loss: 0.7853456426061978 \t\t Validation Loss: 0.8204459488391876\n",
      "Epoch 49 \t\t Training Loss: 0.8016491290284784 \t\t Validation Loss: 0.799116188287735\n",
      "Epoch 50 \t\t Training Loss: 0.7593998395276992 \t\t Validation Loss: 0.7896199107170105\n",
      "Epoch 51 \t\t Training Loss: 0.7588510309135058 \t\t Validation Loss: 0.8244972348213195\n",
      "Epoch 52 \t\t Training Loss: 0.7714545654328488 \t\t Validation Loss: 0.8085027456283569\n",
      "Epoch 53 \t\t Training Loss: 0.7813884339279892 \t\t Validation Loss: 0.7784904420375824\n",
      "Epoch 54 \t\t Training Loss: 0.7471287657213475 \t\t Validation Loss: 0.7955398857593536\n",
      "Epoch 55 \t\t Training Loss: 0.7715873925725399 \t\t Validation Loss: 0.7693191230297088\n",
      "Epoch 56 \t\t Training Loss: 0.7400868330871203 \t\t Validation Loss: 0.8103500962257385\n",
      "Epoch 57 \t\t Training Loss: 0.75881535564009 \t\t Validation Loss: 0.7588982105255127\n",
      "Epoch 58 \t\t Training Loss: 0.7246201150325122 \t\t Validation Loss: 0.8216015219688415\n",
      "Epoch 59 \t\t Training Loss: 0.7616193399903524 \t\t Validation Loss: 0.771023827791214\n",
      "Epoch 60 \t\t Training Loss: 0.7347357385066333 \t\t Validation Loss: 0.7542499661445617\n",
      "Epoch 61 \t\t Training Loss: 0.7237683159211723 \t\t Validation Loss: 0.7592369854450226\n",
      "Epoch 62 \t\t Training Loss: 0.7213029015130101 \t\t Validation Loss: 0.9460683465003967\n",
      "Epoch 63 \t\t Training Loss: 0.8337791786009435 \t\t Validation Loss: 0.7885248064994812\n",
      "Epoch 64 \t\t Training Loss: 0.7556349951259339 \t\t Validation Loss: 0.7750509679317474\n",
      "Epoch 65 \t\t Training Loss: 0.72923043419643 \t\t Validation Loss: 0.750515478849411\n",
      "Epoch 66 \t\t Training Loss: 0.7176551287016157 \t\t Validation Loss: 0.7637710571289062\n",
      "Epoch 67 \t\t Training Loss: 0.7171396221245191 \t\t Validation Loss: 0.7475919902324677\n",
      "Epoch 68 \t\t Training Loss: 0.7613844053192034 \t\t Validation Loss: 0.760331803560257\n",
      "Epoch 69 \t\t Training Loss: 0.7232244778733227 \t\t Validation Loss: 0.7726625204086304\n",
      "Epoch 70 \t\t Training Loss: 0.8139762996968644 \t\t Validation Loss: 0.8112156748771667\n",
      "Epoch 71 \t\t Training Loss: 0.7676303528290427 \t\t Validation Loss: 0.7877877473831176\n",
      "Epoch 72 \t\t Training Loss: 0.7761818107979074 \t\t Validation Loss: 0.7774969279766083\n",
      "Epoch 73 \t\t Training Loss: 0.7488811342097119 \t\t Validation Loss: 0.8913178324699402\n",
      "Epoch 74 \t\t Training Loss: 0.7926159952226923 \t\t Validation Loss: 0.7700889945030213\n",
      "Epoch 75 \t\t Training Loss: 0.7389088411357522 \t\t Validation Loss: 0.7633231163024903\n",
      "Epoch 76 \t\t Training Loss: 0.7257092996855468 \t\t Validation Loss: 0.8131153345108032\n",
      "Epoch 77 \t\t Training Loss: 0.7542676211062057 \t\t Validation Loss: 0.7569206833839417\n",
      "Epoch 78 \t\t Training Loss: 0.7206881329499556 \t\t Validation Loss: 0.7447829961776733\n",
      "Epoch 79 \t\t Training Loss: 0.7086918051071589 \t\t Validation Loss: 0.7427783608436584\n",
      "Epoch 80 \t\t Training Loss: 0.7077074706225105 \t\t Validation Loss: 0.7454140245914459\n",
      "Epoch 81 \t\t Training Loss: 0.7253195511702016 \t\t Validation Loss: 0.7414735436439515\n",
      "Epoch 82 \t\t Training Loss: 0.7005437764014987 \t\t Validation Loss: 0.7643430888652801\n",
      "Epoch 83 \t\t Training Loss: 0.7286880543218792 \t\t Validation Loss: 0.7539881050586701\n",
      "Epoch 84 \t\t Training Loss: 0.7231002123974963 \t\t Validation Loss: 0.7409559071063996\n",
      "Epoch 85 \t\t Training Loss: 0.7306503686457049 \t\t Validation Loss: 0.7563464164733886\n",
      "Epoch 86 \t\t Training Loss: 0.7179308965061251 \t\t Validation Loss: 0.7284858405590058\n",
      "Epoch 87 \t\t Training Loss: 0.6895929467118247 \t\t Validation Loss: 0.7240715444087982\n",
      "Epoch 88 \t\t Training Loss: 0.6884861822286363 \t\t Validation Loss: 0.7358609497547149\n",
      "Epoch 89 \t\t Training Loss: 0.7084728150077946 \t\t Validation Loss: 0.756980174779892\n",
      "Epoch 90 \t\t Training Loss: 0.7274437556609264 \t\t Validation Loss: 0.740249902009964\n",
      "Epoch 91 \t\t Training Loss: 0.7089163473297878 \t\t Validation Loss: 0.8394702792167663\n",
      "Epoch 92 \t\t Training Loss: 0.761671134780125 \t\t Validation Loss: 0.7466300010681153\n",
      "Epoch 93 \t\t Training Loss: 0.7180266640462928 \t\t Validation Loss: 0.7285157978534699\n",
      "Epoch 94 \t\t Training Loss: 0.6879883365736482 \t\t Validation Loss: 0.7396412491798401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 \t\t Training Loss: 0.6968542867602564 \t\t Validation Loss: 0.720846700668335\n",
      "Epoch 96 \t\t Training Loss: 0.6901498035172731 \t\t Validation Loss: 0.7130887567996979\n",
      "Epoch 97 \t\t Training Loss: 0.6752659683398778 \t\t Validation Loss: 0.7341434180736541\n",
      "Epoch 98 \t\t Training Loss: 0.6991206829060507 \t\t Validation Loss: 0.7255140006542206\n",
      "Epoch 99 \t\t Training Loss: 0.6809183464524495 \t\t Validation Loss: 0.724031400680542\n",
      "Epoch 100 \t\t Training Loss: 0.6922672450213142 \t\t Validation Loss: 0.7333631634712219\n",
      "Epoch 101 \t\t Training Loss: 0.6971494354595795 \t\t Validation Loss: 0.7223145723342895\n",
      "Epoch 102 \t\t Training Loss: 0.6777296682089073 \t\t Validation Loss: 0.7481079459190368\n",
      "Epoch 103 \t\t Training Loss: 0.7274628475884706 \t\t Validation Loss: 0.7190742790699005\n",
      "Epoch 104 \t\t Training Loss: 0.6747715142221082 \t\t Validation Loss: 0.7091256380081177\n",
      "Epoch 105 \t\t Training Loss: 0.6728539061809772 \t\t Validation Loss: 0.8138749241828919\n",
      "Epoch 106 \t\t Training Loss: 0.7946546104072866 \t\t Validation Loss: 0.7668875336647034\n",
      "Epoch 107 \t\t Training Loss: 0.7157874160050028 \t\t Validation Loss: 0.7873418152332305\n",
      "Epoch 108 \t\t Training Loss: 0.7350218477170112 \t\t Validation Loss: 0.74363973736763\n",
      "Epoch 109 \t\t Training Loss: 0.7403728570068738 \t\t Validation Loss: 0.7569839358329773\n",
      "Epoch 110 \t\t Training Loss: 0.7295365676036856 \t\t Validation Loss: 0.731014347076416\n",
      "Epoch 111 \t\t Training Loss: 0.6925538057945051 \t\t Validation Loss: 0.728071391582489\n",
      "Epoch 112 \t\t Training Loss: 0.68791480946936 \t\t Validation Loss: 0.8069782376289367\n",
      "Epoch 113 \t\t Training Loss: 0.74672037346587 \t\t Validation Loss: 0.7507568776607514\n",
      "Epoch 114 \t\t Training Loss: 0.6900789112017299 \t\t Validation Loss: 0.7370437741279602\n",
      "Epoch 115 \t\t Training Loss: 0.7565721451248253 \t\t Validation Loss: 0.7398353934288024\n",
      "Epoch 116 \t\t Training Loss: 0.6956107379323211 \t\t Validation Loss: 0.7391930758953095\n",
      "Epoch 117 \t\t Training Loss: 0.7053638525430669 \t\t Validation Loss: 0.7136119663715362\n",
      "Epoch 118 \t\t Training Loss: 0.6675905281502897 \t\t Validation Loss: 0.7251809656620025\n",
      "Epoch 119 \t\t Training Loss: 0.6787734914221157 \t\t Validation Loss: 0.7031521022319793\n",
      "Epoch 120 \t\t Training Loss: 0.662205347369389 \t\t Validation Loss: 0.753941434621811\n",
      "Epoch 121 \t\t Training Loss: 0.7276787402221511 \t\t Validation Loss: 0.7701883673667907\n",
      "Epoch 122 \t\t Training Loss: 0.7060445066315034 \t\t Validation Loss: 0.7198499798774719\n",
      "Epoch 123 \t\t Training Loss: 0.6836565327907794 \t\t Validation Loss: 0.7434912204742432\n",
      "Epoch 124 \t\t Training Loss: 0.6955829723763861 \t\t Validation Loss: 0.7079382598400116\n",
      "Epoch 125 \t\t Training Loss: 0.6630950000404653 \t\t Validation Loss: 0.7161670565605164\n",
      "Epoch 126 \t\t Training Loss: 0.7092421634421164 \t\t Validation Loss: 0.8118598341941834\n",
      "Epoch 127 \t\t Training Loss: 0.6989106432179719 \t\t Validation Loss: 0.7142298638820648\n",
      "Epoch 128 \t\t Training Loss: 0.6877132611380098 \t\t Validation Loss: 0.742594301700592\n",
      "Epoch 129 \t\t Training Loss: 0.6931439351640354 \t\t Validation Loss: 0.6997811198234558\n",
      "Epoch 130 \t\t Training Loss: 0.6616862770272882 \t\t Validation Loss: 0.6998283565044403\n",
      "Epoch 131 \t\t Training Loss: 0.6538509193046317 \t\t Validation Loss: 0.6937429726123809\n",
      "Epoch 132 \t\t Training Loss: 0.6782955695252392 \t\t Validation Loss: 0.7260514914989471\n",
      "Epoch 133 \t\t Training Loss: 0.7019537557225201 \t\t Validation Loss: 0.7211259543895722\n",
      "Epoch 134 \t\t Training Loss: 0.701296167808343 \t\t Validation Loss: 0.7272545099258423\n",
      "Epoch 135 \t\t Training Loss: 0.6831573542639695 \t\t Validation Loss: 0.7136084735393524\n",
      "Epoch 136 \t\t Training Loss: 0.675387563955718 \t\t Validation Loss: 0.7099214553833008\n",
      "Epoch 137 \t\t Training Loss: 0.6740555756658481 \t\t Validation Loss: 0.7076752364635468\n",
      "Epoch 138 \t\t Training Loss: 0.6593056601055419 \t\t Validation Loss: 0.7083115577697754\n",
      "Epoch 139 \t\t Training Loss: 0.672417339946025 \t\t Validation Loss: 0.6922871708869934\n",
      "Epoch 140 \t\t Training Loss: 0.6462998384434874 \t\t Validation Loss: 0.6891812801361084\n",
      "Epoch 141 \t\t Training Loss: 0.6432944299107757 \t\t Validation Loss: 0.7002708911895752\n",
      "Epoch 142 \t\t Training Loss: 0.6560740181095692 \t\t Validation Loss: 0.6874089121818543\n",
      "Epoch 143 \t\t Training Loss: 0.6384661051120547 \t\t Validation Loss: 0.6782032668590545\n",
      "Epoch 144 \t\t Training Loss: 0.6397093070146128 \t\t Validation Loss: 0.7922402799129487\n",
      "Epoch 145 \t\t Training Loss: 0.7408853313211579 \t\t Validation Loss: 0.7175608694553375\n",
      "Epoch 146 \t\t Training Loss: 0.6691848793741089 \t\t Validation Loss: 0.6958589494228363\n",
      "Epoch 147 \t\t Training Loss: 0.6458496373990623 \t\t Validation Loss: 0.7000192940235138\n",
      "Epoch 148 \t\t Training Loss: 0.6436249281161398 \t\t Validation Loss: 0.7569407641887664\n",
      "Epoch 149 \t\t Training Loss: 0.6811120062243214 \t\t Validation Loss: 0.6961275935173035\n",
      "Epoch 150 \t\t Training Loss: 0.6911519556743664 \t\t Validation Loss: 0.7030095517635345\n",
      "Epoch 151 \t\t Training Loss: 0.6616102951007653 \t\t Validation Loss: 0.7188093185424804\n",
      "Epoch 152 \t\t Training Loss: 0.6792243277828997 \t\t Validation Loss: 0.7428682625293732\n",
      "Epoch 153 \t\t Training Loss: 0.7814796659169276 \t\t Validation Loss: 0.7588341712951661\n",
      "Epoch 154 \t\t Training Loss: 0.7269409028864697 \t\t Validation Loss: 0.7481516540050507\n",
      "Epoch 155 \t\t Training Loss: 0.7155521396773955 \t\t Validation Loss: 0.7077114284038544\n",
      "Epoch 156 \t\t Training Loss: 0.669969176060587 \t\t Validation Loss: 0.6934431731700897\n",
      "Epoch 157 \t\t Training Loss: 0.6639397312264416 \t\t Validation Loss: 0.7157699525356293\n",
      "Epoch 158 \t\t Training Loss: 0.6577617536756873 \t\t Validation Loss: 0.6832379817962646\n",
      "Epoch 159 \t\t Training Loss: 0.6439243818514914 \t\t Validation Loss: 0.7324465692043305\n",
      "Epoch 160 \t\t Training Loss: 0.6848772303175531 \t\t Validation Loss: 0.6874535977840424\n",
      "Epoch 161 \t\t Training Loss: 0.6441328568353178 \t\t Validation Loss: 0.6801187098026276\n",
      "Epoch 162 \t\t Training Loss: 0.640510049972745 \t\t Validation Loss: 0.7017263174057007\n",
      "Epoch 163 \t\t Training Loss: 0.657915217112441 \t\t Validation Loss: 0.6969853460788726\n",
      "Epoch 164 \t\t Training Loss: 0.6690073941952616 \t\t Validation Loss: 0.6890041112899781\n",
      "Epoch 165 \t\t Training Loss: 0.6404346389006514 \t\t Validation Loss: 0.7317966103553772\n",
      "Epoch 166 \t\t Training Loss: 0.6795470447830074 \t\t Validation Loss: 0.7026282727718354\n",
      "Epoch 167 \t\t Training Loss: 0.6541659680519315 \t\t Validation Loss: 0.6760570526123046\n",
      "Epoch 168 \t\t Training Loss: 0.631169298406464 \t\t Validation Loss: 0.7019174098968506\n",
      "Epoch 169 \t\t Training Loss: 0.6390241237552785 \t\t Validation Loss: 0.6704967439174652\n",
      "Epoch 170 \t\t Training Loss: 0.6226540347818512 \t\t Validation Loss: 0.6696035325527191\n",
      "Epoch 171 \t\t Training Loss: 0.6263726650022011 \t\t Validation Loss: 0.8577432513237\n",
      "Epoch 172 \t\t Training Loss: 0.7277884806058683 \t\t Validation Loss: 0.6943438053131104\n",
      "Epoch 173 \t\t Training Loss: 0.6433500522408038 \t\t Validation Loss: 0.686074435710907\n",
      "Epoch 174 \t\t Training Loss: 0.6530838944635339 \t\t Validation Loss: 0.6865920841693878\n",
      "Epoch 175 \t\t Training Loss: 0.6353843449229035 \t\t Validation Loss: 0.6913917601108551\n",
      "Epoch 176 \t\t Training Loss: 0.6431053086539 \t\t Validation Loss: 0.6890298902988434\n",
      "Epoch 177 \t\t Training Loss: 0.6313556454128982 \t\t Validation Loss: 0.6786824584007263\n",
      "Epoch 178 \t\t Training Loss: 0.6240301032586651 \t\t Validation Loss: 0.6955352902412415\n",
      "Epoch 179 \t\t Training Loss: 0.6310614232200286 \t\t Validation Loss: 0.6942193150520325\n",
      "Epoch 180 \t\t Training Loss: 0.6428901273242676 \t\t Validation Loss: 0.6988979399204254\n",
      "Epoch 181 \t\t Training Loss: 0.6468489825396248 \t\t Validation Loss: 0.7151263535022736\n",
      "Epoch 182 \t\t Training Loss: 0.640091806485508 \t\t Validation Loss: 0.6752785921096802\n",
      "Epoch 183 \t\t Training Loss: 0.6336185135235444 \t\t Validation Loss: 0.7063549637794495\n",
      "Epoch 184 \t\t Training Loss: 0.6387194658511252 \t\t Validation Loss: 0.6679044902324677\n",
      "Epoch 185 \t\t Training Loss: 0.6150896126888075 \t\t Validation Loss: 0.6640995919704438\n",
      "Epoch 186 \t\t Training Loss: 0.6192792815398116 \t\t Validation Loss: 0.735488760471344\n",
      "Epoch 187 \t\t Training Loss: 0.7566959616722981 \t\t Validation Loss: 0.7208949267864228\n",
      "Epoch 188 \t\t Training Loss: 0.6560795998705026 \t\t Validation Loss: 0.7220273733139038\n",
      "Epoch 189 \t\t Training Loss: 0.6652249381028487 \t\t Validation Loss: 0.7386962532997131\n",
      "Epoch 190 \t\t Training Loss: 0.7051194442734534 \t\t Validation Loss: 0.6857820391654968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 \t\t Training Loss: 0.6313749466153139 \t\t Validation Loss: 0.6946754574775695\n",
      "Epoch 192 \t\t Training Loss: 0.6487259808824866 \t\t Validation Loss: 0.7168311595916748\n",
      "Epoch 193 \t\t Training Loss: 0.675157459401294 \t\t Validation Loss: 0.744129729270935\n",
      "Epoch 194 \t\t Training Loss: 0.6785894497323431 \t\t Validation Loss: 0.6800512552261353\n",
      "Epoch 195 \t\t Training Loss: 0.6419070481595414 \t\t Validation Loss: 0.6751221299171448\n",
      "Epoch 196 \t\t Training Loss: 0.6380288485664031 \t\t Validation Loss: 0.6773589074611663\n",
      "Epoch 197 \t\t Training Loss: 0.6334475548886462 \t\t Validation Loss: 0.6694997191429138\n",
      "Epoch 198 \t\t Training Loss: 0.6327891458463932 \t\t Validation Loss: 0.6630916714668273\n",
      "Epoch 199 \t\t Training Loss: 0.6199047828906149 \t\t Validation Loss: 0.662360692024231\n",
      "Epoch 200 \t\t Training Loss: 0.6119544160300197 \t\t Validation Loss: 0.6865121066570282\n",
      "['EVA' 'ELC' 'ZBH' 'BYM' 'PCAR' 'ROST' 'ISRG' 'DRI' 'GPC' 'HOLX' 'SPH'\n",
      " 'AAP' 'EL' 'BCEI' 'ADX' 'BK' 'PLYM' 'ICE' 'HRL' 'SCHW' 'MMM' 'AOS' 'PNR'\n",
      " 'CL' 'SLCA' 'FANG' 'CELG~' 'ACN' 'TNK' 'DPW' 'HUBB' 'PAYC' 'JNJ' 'IQV'\n",
      " 'MEG']\n",
      "Exception on BCEI:\n",
      "'high'\n",
      "Exception on CELG~:\n",
      "'high'\n",
      "torch.Size([1184350, 58])\n",
      "Batch size: 8282\n",
      "Epoch 1 \t\t Training Loss: 0.9968204100926717 \t\t Validation Loss: 0.9614519998431206\n",
      "Epoch 2 \t\t Training Loss: 0.9691914750470055 \t\t Validation Loss: 0.95134686678648\n",
      "Epoch 3 \t\t Training Loss: 0.9554587557084031 \t\t Validation Loss: 0.942362904548645\n",
      "Epoch 4 \t\t Training Loss: 0.9424381285077996 \t\t Validation Loss: 0.9245122224092484\n",
      "Epoch 5 \t\t Training Loss: 0.9312480551501116 \t\t Validation Loss: 0.9128272458910942\n",
      "Epoch 6 \t\t Training Loss: 0.9138274424605899 \t\t Validation Loss: 0.9058570042252541\n",
      "Epoch 7 \t\t Training Loss: 0.9061046842899587 \t\t Validation Loss: 0.8960943967103958\n",
      "Epoch 8 \t\t Training Loss: 0.8885615120331446 \t\t Validation Loss: 0.8807352110743523\n",
      "Epoch 9 \t\t Training Loss: 0.8791067525744438 \t\t Validation Loss: 0.8749773353338242\n",
      "Epoch 10 \t\t Training Loss: 0.8688403243819872 \t\t Validation Loss: 0.8660032972693443\n",
      "Epoch 11 \t\t Training Loss: 0.8577707898285654 \t\t Validation Loss: 0.8577894493937492\n",
      "Epoch 12 \t\t Training Loss: 0.8475057892501354 \t\t Validation Loss: 0.8507585376501083\n",
      "Epoch 13 \t\t Training Loss: 0.8374138321313593 \t\t Validation Loss: 0.8408881723880768\n",
      "Epoch 14 \t\t Training Loss: 0.8285369339088599 \t\t Validation Loss: 0.8411486446857452\n",
      "Epoch 15 \t\t Training Loss: 0.820728737860918 \t\t Validation Loss: 0.8390964940190315\n",
      "Epoch 16 \t\t Training Loss: 0.8225808122919666 \t\t Validation Loss: 0.8306434601545334\n",
      "Epoch 17 \t\t Training Loss: 0.81454415867726 \t\t Validation Loss: 0.8227636143565178\n",
      "Epoch 18 \t\t Training Loss: 0.8350661889546447 \t\t Validation Loss: 0.8286571055650711\n",
      "Epoch 19 \t\t Training Loss: 0.8189603665636646 \t\t Validation Loss: 0.8081416338682175\n",
      "Epoch 20 \t\t Training Loss: 0.7931764473517736 \t\t Validation Loss: 0.8042958974838257\n",
      "Epoch 21 \t\t Training Loss: 0.7983340832094351 \t\t Validation Loss: 0.8018410578370094\n",
      "Epoch 22 \t\t Training Loss: 0.783771985106998 \t\t Validation Loss: 0.7915429249405861\n",
      "Epoch 23 \t\t Training Loss: 0.7763310302462842 \t\t Validation Loss: 0.813096284866333\n",
      "Epoch 24 \t\t Training Loss: 0.7996588514910804 \t\t Validation Loss: 0.7957659512758255\n",
      "Epoch 25 \t\t Training Loss: 0.7777906515532069 \t\t Validation Loss: 0.7817056328058243\n",
      "Epoch 26 \t\t Training Loss: 0.7600768746601211 \t\t Validation Loss: 0.7930066958069801\n",
      "Epoch 27 \t\t Training Loss: 0.773928001936939 \t\t Validation Loss: 0.7765863686800003\n",
      "Epoch 28 \t\t Training Loss: 0.7628538217395544 \t\t Validation Loss: 0.7770920842885971\n",
      "Epoch 29 \t\t Training Loss: 0.7618988752365112 \t\t Validation Loss: 0.7673805803060532\n",
      "Epoch 30 \t\t Training Loss: 0.7466130732662148 \t\t Validation Loss: 0.7640755772590637\n",
      "Epoch 31 \t\t Training Loss: 0.7519697472453117 \t\t Validation Loss: 0.767693005502224\n",
      "Epoch 32 \t\t Training Loss: 0.7390789534482691 \t\t Validation Loss: 0.7588487938046455\n",
      "Epoch 33 \t\t Training Loss: 0.7428319269998206 \t\t Validation Loss: 0.7525601163506508\n",
      "Epoch 34 \t\t Training Loss: 0.7278126142919064 \t\t Validation Loss: 0.7586594298481941\n",
      "Epoch 35 \t\t Training Loss: 0.7250858392152522 \t\t Validation Loss: 0.7494093030691147\n",
      "Epoch 36 \t\t Training Loss: 0.7258580111795001 \t\t Validation Loss: 0.7490416765213013\n",
      "Epoch 37 \t\t Training Loss: 0.7206062115728855 \t\t Validation Loss: 0.7386067658662796\n",
      "Epoch 38 \t\t Training Loss: 0.7121505969100528 \t\t Validation Loss: 0.7484955340623856\n",
      "Epoch 39 \t\t Training Loss: 0.7153441711432404 \t\t Validation Loss: 0.7325913682579994\n",
      "Epoch 40 \t\t Training Loss: 0.7187990239924855 \t\t Validation Loss: 0.7330111339688301\n",
      "Epoch 41 \t\t Training Loss: 0.6998068263961209 \t\t Validation Loss: 0.7263894751667976\n",
      "Epoch 42 \t\t Training Loss: 0.6975750277439753 \t\t Validation Loss: 0.7233152985572815\n",
      "Epoch 43 \t\t Training Loss: 0.7110836191309823 \t\t Validation Loss: 0.7310708239674568\n",
      "Epoch 44 \t\t Training Loss: 0.7217656067676015 \t\t Validation Loss: 0.7299471497535706\n",
      "Epoch 45 \t\t Training Loss: 0.6954438793990347 \t\t Validation Loss: 0.7286417335271835\n",
      "Epoch 46 \t\t Training Loss: 0.6910860215624174 \t\t Validation Loss: 0.7497699335217476\n",
      "Epoch 47 \t\t Training Loss: 0.7044108998444345 \t\t Validation Loss: 0.7524354532361031\n",
      "Epoch 48 \t\t Training Loss: 0.7194748344934649 \t\t Validation Loss: 0.7270944342017174\n",
      "Epoch 49 \t\t Training Loss: 0.6986715735660659 \t\t Validation Loss: 0.7287183701992035\n",
      "Epoch 50 \t\t Training Loss: 0.6893313088350825 \t\t Validation Loss: 0.725177988409996\n",
      "Epoch 51 \t\t Training Loss: 0.6994129431744417 \t\t Validation Loss: 0.7169998735189438\n",
      "Epoch 52 \t\t Training Loss: 0.6811837868558036 \t\t Validation Loss: 0.7118869200348854\n",
      "Epoch 53 \t\t Training Loss: 0.6767422457536062 \t\t Validation Loss: 0.7114417403936386\n",
      "Epoch 54 \t\t Training Loss: 0.6741619561281469 \t\t Validation Loss: 0.7150832638144493\n",
      "Epoch 55 \t\t Training Loss: 0.6895236857235432 \t\t Validation Loss: 0.7071495056152344\n",
      "Epoch 56 \t\t Training Loss: 0.6765349915044175 \t\t Validation Loss: 0.704083725810051\n",
      "Epoch 57 \t\t Training Loss: 0.675283377783166 \t\t Validation Loss: 0.7262218073010445\n",
      "Epoch 58 \t\t Training Loss: 0.7597949782179462 \t\t Validation Loss: 0.7270016744732857\n",
      "Epoch 59 \t\t Training Loss: 0.6998075946337647 \t\t Validation Loss: 0.713465541601181\n",
      "Epoch 60 \t\t Training Loss: 0.6935590108235677 \t\t Validation Loss: 0.7009396627545357\n",
      "Epoch 61 \t\t Training Loss: 0.6699740712841352 \t\t Validation Loss: 0.7101482376456261\n",
      "Epoch 62 \t\t Training Loss: 0.7002395064466529 \t\t Validation Loss: 0.7118356972932816\n",
      "Epoch 63 \t\t Training Loss: 0.6813667441407839 \t\t Validation Loss: 0.6997816860675812\n",
      "Epoch 64 \t\t Training Loss: 0.6687308483653598 \t\t Validation Loss: 0.6913294121623039\n",
      "Epoch 65 \t\t Training Loss: 0.6533803476227654 \t\t Validation Loss: 0.7097215875983238\n",
      "Epoch 66 \t\t Training Loss: 0.6750965201192431 \t\t Validation Loss: 0.6889540627598763\n",
      "Epoch 67 \t\t Training Loss: 0.6529263659483857 \t\t Validation Loss: 0.6930498853325844\n",
      "Epoch 68 \t\t Training Loss: 0.6608424927625391 \t\t Validation Loss: 0.6882657781243324\n",
      "Epoch 69 \t\t Training Loss: 0.6499441125326686 \t\t Validation Loss: 0.6823570430278778\n",
      "Epoch 70 \t\t Training Loss: 0.6468903569297658 \t\t Validation Loss: 0.6933874264359474\n",
      "Epoch 71 \t\t Training Loss: 0.6546387879384888 \t\t Validation Loss: 0.6865533217787743\n",
      "Epoch 72 \t\t Training Loss: 0.653256528907352 \t\t Validation Loss: 0.6831378936767578\n",
      "Epoch 73 \t\t Training Loss: 0.6409230625463856 \t\t Validation Loss: 0.6738805919885635\n",
      "Epoch 74 \t\t Training Loss: 0.6367828283045027 \t\t Validation Loss: 0.6858339384198189\n",
      "Epoch 75 \t\t Training Loss: 0.6508025245534049 \t\t Validation Loss: 0.676490306854248\n",
      "Epoch 76 \t\t Training Loss: 0.6410285660790073 \t\t Validation Loss: 0.679181195795536\n",
      "Epoch 77 \t\t Training Loss: 0.6545331883761618 \t\t Validation Loss: 0.6743519529700279\n",
      "Epoch 78 \t\t Training Loss: 0.6366962045431137 \t\t Validation Loss: 0.6883435696363449\n",
      "Epoch 79 \t\t Training Loss: 0.6450509735279613 \t\t Validation Loss: 0.6880155205726624\n",
      "Epoch 80 \t\t Training Loss: 0.6760457538896136 \t\t Validation Loss: 0.7032386586070061\n",
      "Epoch 81 \t\t Training Loss: 0.6482121280084053 \t\t Validation Loss: 0.6745106056332588\n",
      "Epoch 82 \t\t Training Loss: 0.6475583476324877 \t\t Validation Loss: 0.6672948151826859\n",
      "Epoch 83 \t\t Training Loss: 0.6308475670715173 \t\t Validation Loss: 0.6803949028253555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 \t\t Training Loss: 0.6348426037778457 \t\t Validation Loss: 0.6671542823314667\n",
      "Epoch 85 \t\t Training Loss: 0.6233250080711312 \t\t Validation Loss: 0.6894032061100006\n",
      "Epoch 86 \t\t Training Loss: 0.632627888272206 \t\t Validation Loss: 0.6690581142902374\n",
      "Epoch 87 \t\t Training Loss: 0.6409851548572382 \t\t Validation Loss: 0.6638755202293396\n",
      "Epoch 88 \t\t Training Loss: 0.6277548054026233 \t\t Validation Loss: 0.6936742663383484\n",
      "Epoch 89 \t\t Training Loss: 0.6486781450609366 \t\t Validation Loss: 0.6677598655223846\n",
      "Epoch 90 \t\t Training Loss: 0.6325305712719759 \t\t Validation Loss: 0.6656330153346062\n",
      "Epoch 91 \t\t Training Loss: 0.6312589707473913 \t\t Validation Loss: 0.6714325845241547\n",
      "Epoch 92 \t\t Training Loss: 0.6438807021412585 \t\t Validation Loss: 0.6646020486950874\n",
      "Epoch 93 \t\t Training Loss: 0.6299462521241771 \t\t Validation Loss: 0.6786826476454735\n",
      "Epoch 94 \t\t Training Loss: 0.6365643044312795 \t\t Validation Loss: 0.6597863882780075\n",
      "Epoch 95 \t\t Training Loss: 0.6281863330966897 \t\t Validation Loss: 0.6617806777358055\n",
      "Epoch 96 \t\t Training Loss: 0.6275624059554603 \t\t Validation Loss: 0.6548679545521736\n",
      "Epoch 97 \t\t Training Loss: 0.6134291833473576 \t\t Validation Loss: 0.6648703515529633\n",
      "Epoch 98 \t\t Training Loss: 0.6596053693857458 \t\t Validation Loss: 0.6642602607607841\n",
      "Epoch 99 \t\t Training Loss: 0.6306793466210365 \t\t Validation Loss: 0.6559841483831406\n",
      "Epoch 100 \t\t Training Loss: 0.6149831203122934 \t\t Validation Loss: 0.6493798494338989\n",
      "Epoch 101 \t\t Training Loss: 0.6158162065678172 \t\t Validation Loss: 0.6544245630502701\n",
      "Epoch 102 \t\t Training Loss: 0.621670270131694 \t\t Validation Loss: 0.6583947390317917\n",
      "Epoch 103 \t\t Training Loss: 0.6428409831391441 \t\t Validation Loss: 0.6508653163909912\n",
      "Epoch 104 \t\t Training Loss: 0.6098334607150819 \t\t Validation Loss: 0.6552418693900108\n",
      "Epoch 105 \t\t Training Loss: 0.6237781321009 \t\t Validation Loss: 0.6708916276693344\n",
      "Epoch 106 \t\t Training Loss: 0.6335753554271327 \t\t Validation Loss: 0.6639880388975143\n",
      "Epoch 107 \t\t Training Loss: 0.6651631461249458 \t\t Validation Loss: 0.675557404756546\n",
      "Epoch 108 \t\t Training Loss: 0.6257613251606623 \t\t Validation Loss: 0.6714588031172752\n",
      "Epoch 109 \t\t Training Loss: 0.6246285393006272 \t\t Validation Loss: 0.6721933409571648\n",
      "Epoch 110 \t\t Training Loss: 0.6132632419466972 \t\t Validation Loss: 0.6634919196367264\n",
      "Epoch 111 \t\t Training Loss: 0.6277185185915894 \t\t Validation Loss: 0.6469593048095703\n",
      "Epoch 112 \t\t Training Loss: 0.6003663030763468 \t\t Validation Loss: 0.6436404436826706\n",
      "Epoch 113 \t\t Training Loss: 0.6047251108619902 \t\t Validation Loss: 0.6566060706973076\n",
      "Epoch 114 \t\t Training Loss: 0.61304796859622 \t\t Validation Loss: 0.6420734897255898\n",
      "Epoch 115 \t\t Training Loss: 0.6009335112240579 \t\t Validation Loss: 0.6418002918362617\n",
      "Epoch 116 \t\t Training Loss: 0.603844779647059 \t\t Validation Loss: 0.6404390931129456\n",
      "Epoch 117 \t\t Training Loss: 0.5938200900952021 \t\t Validation Loss: 0.6446327120065689\n",
      "Epoch 118 \t\t Training Loss: 0.5951482630852196 \t\t Validation Loss: 0.6417748257517815\n",
      "Epoch 119 \t\t Training Loss: 0.6092106360528204 \t\t Validation Loss: 0.6396731212735176\n",
      "Epoch 120 \t\t Training Loss: 0.594066594623857 \t\t Validation Loss: 0.6409566551446915\n",
      "Epoch 121 \t\t Training Loss: 0.5969893561883105 \t\t Validation Loss: 0.6364947333931923\n",
      "Epoch 122 \t\t Training Loss: 0.592428718176153 \t\t Validation Loss: 0.6523115634918213\n",
      "Epoch 123 \t\t Training Loss: 0.6088618151843548 \t\t Validation Loss: 0.6490722298622131\n",
      "Epoch 124 \t\t Training Loss: 0.6105935830208991 \t\t Validation Loss: 0.654671348631382\n",
      "Epoch 125 \t\t Training Loss: 0.5968415894442134 \t\t Validation Loss: 0.6374204978346825\n",
      "Epoch 126 \t\t Training Loss: 0.5854285406983561 \t\t Validation Loss: 0.6347159594297409\n",
      "Epoch 127 \t\t Training Loss: 0.5830890668763055 \t\t Validation Loss: 0.6557036638259888\n",
      "Epoch 128 \t\t Training Loss: 0.604677210872372 \t\t Validation Loss: 0.6438601613044739\n",
      "Epoch 129 \t\t Training Loss: 0.5919815862758292 \t\t Validation Loss: 0.6385551020503044\n",
      "Epoch 130 \t\t Training Loss: 0.5919695898062654 \t\t Validation Loss: 0.6461268216371536\n",
      "Epoch 131 \t\t Training Loss: 0.6359903886914253 \t\t Validation Loss: 0.6401676088571548\n",
      "Epoch 132 \t\t Training Loss: 0.5913902278989553 \t\t Validation Loss: 0.6706609278917313\n",
      "Epoch 133 \t\t Training Loss: 0.6082545402977202 \t\t Validation Loss: 0.6335470601916313\n",
      "Epoch 134 \t\t Training Loss: 0.5931343771517277 \t\t Validation Loss: 0.6530073061585426\n",
      "Epoch 135 \t\t Training Loss: 0.607286896970537 \t\t Validation Loss: 0.6354056820273399\n",
      "Epoch 136 \t\t Training Loss: 0.6142830914921231 \t\t Validation Loss: 0.6370260789990425\n",
      "Epoch 137 \t\t Training Loss: 0.5919495543671979 \t\t Validation Loss: 0.6330404877662659\n",
      "Epoch 138 \t\t Training Loss: 0.5942564842601618 \t\t Validation Loss: 0.6914809942245483\n",
      "Epoch 139 \t\t Training Loss: 0.6375236883759499 \t\t Validation Loss: 0.6437566056847572\n",
      "Epoch 140 \t\t Training Loss: 0.593822984645764 \t\t Validation Loss: 0.6266239285469055\n",
      "Epoch 141 \t\t Training Loss: 0.584225649634997 \t\t Validation Loss: 0.6315099000930786\n",
      "Epoch 142 \t\t Training Loss: 0.600372482298149 \t\t Validation Loss: 0.6272097527980804\n",
      "Epoch 143 \t\t Training Loss: 0.5872437159220377 \t\t Validation Loss: 0.664247915148735\n",
      "Epoch 144 \t\t Training Loss: 0.5933514525079064 \t\t Validation Loss: 0.6241784319281578\n",
      "Epoch 145 \t\t Training Loss: 0.5773063956035508 \t\t Validation Loss: 0.6243613213300705\n",
      "Epoch 146 \t\t Training Loss: 0.5757846782604853 \t\t Validation Loss: 0.6302375122904778\n",
      "Epoch 147 \t\t Training Loss: 0.5774356002608935 \t\t Validation Loss: 0.635570116341114\n",
      "Epoch 148 \t\t Training Loss: 0.6208100964625677 \t\t Validation Loss: 0.6300868093967438\n",
      "Epoch 149 \t\t Training Loss: 0.5832978184852335 \t\t Validation Loss: 0.6246505603194237\n",
      "Epoch 150 \t\t Training Loss: 0.5740626491606236 \t\t Validation Loss: 0.6272850558161736\n",
      "Epoch 151 \t\t Training Loss: 0.5758509582115544 \t\t Validation Loss: 0.6270147860050201\n",
      "Epoch 152 \t\t Training Loss: 0.5894461319678359 \t\t Validation Loss: 0.6330243647098541\n",
      "Epoch 153 \t\t Training Loss: 0.5815706662833691 \t\t Validation Loss: 0.626953199505806\n",
      "Epoch 154 \t\t Training Loss: 0.5760343915058507 \t\t Validation Loss: 0.6457630321383476\n",
      "Epoch 155 \t\t Training Loss: 0.6052585939566294 \t\t Validation Loss: 0.6331445798277855\n",
      "Epoch 156 \t\t Training Loss: 0.5781681259266205 \t\t Validation Loss: 0.623324990272522\n",
      "Epoch 157 \t\t Training Loss: 0.5728556865619289 \t\t Validation Loss: 0.6414127424359322\n",
      "Epoch 158 \t\t Training Loss: 0.5814766815553108 \t\t Validation Loss: 0.6249163001775742\n",
      "Epoch 159 \t\t Training Loss: 0.5783716899653276 \t\t Validation Loss: 0.6236676052212715\n",
      "Epoch 160 \t\t Training Loss: 0.5744964943991767 \t\t Validation Loss: 0.6415379866957664\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunk_tickers)\n\u001b[1;32m     17\u001b[0m dataloader_train, dataloader_val \u001b[38;5;241m=\u001b[39m get_train_val_dataloaders(chunk_tickers, start_date, end_date)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dataloader_train\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dataloader_val\n",
      "Cell \u001b[0;32mIn [5], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, criterion, optimizer, dataloader_train, dataloader_val, epochs, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m local_batch, local_labels \u001b[38;5;129;01min\u001b[39;00m dataloader_train:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#if local_batch.shape[0] != batch_size:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#    print(f\"Wrong train batch size. Skipping batch.\\nThrowing away {local_batch.shape[0]} samples.\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#    continue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     local_batch, local_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, local_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the model \u001b[39;00m\n\u001b[1;32m     15\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m net(local_batch)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "train_device = torch.device(\"cuda\")\n",
    "\n",
    "# Set device for model\n",
    "net = net.to(train_device)\n",
    "\n",
    "# Select optimizerand loss criteria\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Randomize tickers\n",
    "random.shuffle(tickers)\n",
    "\n",
    "chunk_count = 7\n",
    "chunks = np.array_split(tickers, chunk_count)\n",
    "for chunk_tickers in chunks:\n",
    "    print(chunk_tickers)\n",
    "    dataloader_train, dataloader_val = get_train_val_dataloaders(chunk_tickers, start_date, end_date)\n",
    "    train(net, criterion, optimizer, dataloader_train, dataloader_val, epochs=200)\n",
    "    del dataloader_train\n",
    "    del dataloader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "import backtest as bt\n",
    "from strategy import PretrainedModelStrategy, SignalModelStrategy\n",
    "from technical_signals import TechnicalSignalSet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "random.shuffle(tickers)\n",
    "# XXX temporary - need to rework concurrency to be suitable for CUDA\n",
    "# (Must use `spawn` as opposed to `fork` based concurrency I believe - separate OS processes?)\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def predict(net):\n",
    "    return lambda X:\\\n",
    "        net(torch.from_numpy(X).float().cpu()).detach().numpy()\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    print('dataframe')\n",
    "    print(type(df))\n",
    "    print(df)\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "#strategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=0.95, bias=0.2)\n",
    "strategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=4., bias=0.)\n",
    "#strategy = SignalModelStrategy(SVR(), lambda df: TechnicalSignalSet(df, predict_window=14), cutoff=1., bias=0.1)\n",
    "bt.comprehensive_backtest(strategy, tickers[:3], \"2022-10-11\", \"2025-01-01\", plot=True, train_test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictive_model import PredictiveModel\n",
    "from datetime import datetime\n",
    "from model_env import ModelEnv\n",
    "\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "model = PredictiveModel(net, \"TorchMATI-5min\", predict_window, datetime.now())\n",
    "model.save()\n",
    "\n",
    "model_env = ModelEnv.from_model(model, 'My First Test', [{'id': 'rsi'}], None, None, model_code=f\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.LazyLinear(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "ds.save_model_envs([model_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf371f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
