{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ddb307",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74935c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Suppress ta warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Auto reload local files\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Make files in src/ available to notebook\n",
    "import sys\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dbaed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SPY csv, define config\n",
    "spy_constituents = list(pd.read_csv('../../data/spy_constituents.csv', header=0)['Symbol'])\n",
    "random.shuffle(spy_constituents)\n",
    "\n",
    "tickers = spy_constituents\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "predict_window = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d79fa7",
   "metadata": {},
   "source": [
    "## Sync & Load Data, Create Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e44208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on INFO:\n",
      "'high'\n",
      "Exception on BF.B:\n",
      "'high'\n",
      "Exception on PBCT:\n",
      "'high'\n",
      "Exception on VIAC:\n",
      "'high'\n",
      "Exception on KSU:\n",
      "'high'\n",
      "Exception on WLTW:\n",
      "'high'\n",
      "Exception on OGN:\n",
      "Found array with 0 sample(s) (shape=(0, 58)) while a minimum of 1 is required by RobustScaler.\n",
      "Exception on DISCA:\n",
      "'high'\n",
      "Exception on DISCK:\n",
      "'high'\n",
      "Exception on BRK.B:\n",
      "'high'\n",
      "Exception on XLNX:\n",
      "'high'\n"
     ]
    }
   ],
   "source": [
    "# Load the data from db\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datastore as ds\n",
    "from technical_signals import TechnicalSignalSet\n",
    "\n",
    "#ds.download_daily_candlesticks(tickers, start_date, end_date)\n",
    "candlesticks = ds.get_daily_candlesticks(tickers, start_date, end_date)\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        technical_sigs = TechnicalSignalSet(candlesticks[ticker], predict_window)\n",
    "        X, y, Xy_date = technical_sigs.to_xy()\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "    except Exception as ex:\n",
    "        print(f\"Exception on {ticker}:\")\n",
    "        print(ex)\n",
    "\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "y = np.concatenate(ys, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cde4b",
   "metadata": {},
   "source": [
    "## Clean Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c319093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2059741, 58])\n",
      "Batch size: 913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "\n",
    "def round_batch_size(sample_count, approximately, leeway=None):\n",
    "    \"\"\"\n",
    "    Round batch size to a more suitable value. This helps to avoid a\n",
    "    problem where the final batch has a lot of samples, but not enough for\n",
    "    a full batch, leading to many samples being thrown out.\n",
    "\n",
    "    approximately: int, leeway: int\n",
    "      decide on a chunk size around a number, with specified leeway\n",
    "      (leeway defaults to `approximately // 10`).\n",
    "    \"\"\"\n",
    "    if leeway is None:\n",
    "        leeway = approximately // 10\n",
    "    \n",
    "    # Get the number of leftover samples if we use the suggested batch size\n",
    "    best_leftover = sample_count - np.floor(sample_count / approximately) * approximately\n",
    "\n",
    "    # Brute-force search for the value that yeilds the fewest leftovers\n",
    "    # within the given leeway range.\n",
    "    best_chunk_count = approximately\n",
    "    for offset in range(-leeway, leeway):\n",
    "        chunk_size = approximately + offset\n",
    "        leftover = sample_count - np.floor(sample_count / chunk_size) * chunk_size\n",
    "        if leftover < best_leftover:\n",
    "            best_leftover = leftover\n",
    "            best_chunk_count = chunk_size\n",
    "    return best_chunk_count\n",
    "            \n",
    "\n",
    "batch_size = round_batch_size(X_train.shape[0], 1024, leeway=200)\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Convert X, y to torch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.reshape(y_train.shape[0], 1)).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.reshape(y_test.shape[0], 1)).float()\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print('Batch size:', batch_size)\n",
    "\n",
    "# Generators\n",
    "training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader_train = DataLoader(training_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "validation_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "dataloader_test = DataLoader(validation_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Release duplicated memory\n",
    "try:\n",
    "    del X\n",
    "    del y\n",
    "    del Xs\n",
    "    del ys\n",
    "    #del X_train\n",
    "    #del X_test\n",
    "    #del y_train\n",
    "    #del y_test\n",
    "    del X_train_tensor\n",
    "    del X_test_tensor\n",
    "    del y_train_tensor\n",
    "    del y_test_tensor\n",
    "except:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2d5be",
   "metadata": {},
   "source": [
    "## Create the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35154ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 1\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, n_outputs),\n",
    ")\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, n_outputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93d657",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce603e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 1 \t\t Training Loss: 0.905487361739154 \t\t Validation Loss: 0.890648316814605\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 2 \t\t Training Loss: 0.8630136547228061 \t\t Validation Loss: 0.8565230813634348\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 3 \t\t Training Loss: 0.8409128228964191 \t\t Validation Loss: 0.8354569428945443\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 4 \t\t Training Loss: 0.8238397436591973 \t\t Validation Loss: 0.8322162585429461\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 5 \t\t Training Loss: 0.8108888373841846 \t\t Validation Loss: 0.8167028320263107\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 6 \t\t Training Loss: 0.7976253585156267 \t\t Validation Loss: 0.8065330488273347\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 7 \t\t Training Loss: 0.7866451987451191 \t\t Validation Loss: 0.7930825886973347\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 8 \t\t Training Loss: 0.7764490930760493 \t\t Validation Loss: 0.7863991343642611\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 9 \t\t Training Loss: 0.7663392453897194 \t\t Validation Loss: 0.7783518735156116\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 10 \t\t Training Loss: 0.7588090355490286 \t\t Validation Loss: 0.7838933543854976\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 11 \t\t Training Loss: 0.7488622607570018 \t\t Validation Loss: 0.7675188984054019\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 12 \t\t Training Loss: 0.7415178340517635 \t\t Validation Loss: 0.7646339336239484\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 13 \t\t Training Loss: 0.7356001186487048 \t\t Validation Loss: 0.7528657625870876\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 14 \t\t Training Loss: 0.7285078252159393 \t\t Validation Loss: 0.753484336028536\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 15 \t\t Training Loss: 0.7214974895694058 \t\t Validation Loss: 0.7500018323085222\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 16 \t\t Training Loss: 0.7180561843993762 \t\t Validation Loss: 0.7398652954405522\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 17 \t\t Training Loss: 0.7116557682410879 \t\t Validation Loss: 0.7346477822003612\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 18 \t\t Training Loss: 0.7069644561160388 \t\t Validation Loss: 0.730872153285965\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 19 \t\t Training Loss: 0.7023866852934402 \t\t Validation Loss: 0.7304167054088943\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 20 \t\t Training Loss: 0.6987196314266206 \t\t Validation Loss: 0.7289017201419845\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 21 \t\t Training Loss: 0.6961082880297328 \t\t Validation Loss: 0.7316975700427811\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 22 \t\t Training Loss: 0.6917079510443708 \t\t Validation Loss: 0.7247962654824276\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 23 \t\t Training Loss: 0.6885981740125471 \t\t Validation Loss: 0.724739426635651\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 24 \t\t Training Loss: 0.6853107397251019 \t\t Validation Loss: 0.7182303524587259\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 25 \t\t Training Loss: 0.6827850930977036 \t\t Validation Loss: 0.7150163318056509\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 26 \t\t Training Loss: 0.6795224053805837 \t\t Validation Loss: 0.713661928813296\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 27 \t\t Training Loss: 0.6766277809400816 \t\t Validation Loss: 0.7172828661018159\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 28 \t\t Training Loss: 0.6745597021633616 \t\t Validation Loss: 0.709930823856141\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 29 \t\t Training Loss: 0.6713242935933632 \t\t Validation Loss: 0.7126968863950782\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 30 \t\t Training Loss: 0.6691326800837789 \t\t Validation Loss: 0.71228872779831\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 31 \t\t Training Loss: 0.6670126007836004 \t\t Validation Loss: 0.7089379845387432\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 32 \t\t Training Loss: 0.6645628889023227 \t\t Validation Loss: 0.7014065315998883\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 33 \t\t Training Loss: 0.6618881289495215 \t\t Validation Loss: 0.6989158831269617\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 34 \t\t Training Loss: 0.6608400367560194 \t\t Validation Loss: 0.7023351040494394\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 35 \t\t Training Loss: 0.6581663487229032 \t\t Validation Loss: 0.69893583263534\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 36 \t\t Training Loss: 0.6570584521813365 \t\t Validation Loss: 0.7008355172031904\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 37 \t\t Training Loss: 0.6556053357310246 \t\t Validation Loss: 0.6940146212083885\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 38 \t\t Training Loss: 0.6535138904018427 \t\t Validation Loss: 0.6930735237094986\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 39 \t\t Training Loss: 0.651150892701992 \t\t Validation Loss: 0.6958901934414746\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 40 \t\t Training Loss: 0.6504348631606569 \t\t Validation Loss: 0.6926982528659927\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 41 \t\t Training Loss: 0.6492172100663344 \t\t Validation Loss: 0.6887786096310711\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 42 \t\t Training Loss: 0.6468178550516761 \t\t Validation Loss: 0.689438615899637\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 43 \t\t Training Loss: 0.6450326783127842 \t\t Validation Loss: 0.6914244272794382\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 44 \t\t Training Loss: 0.6442718156999225 \t\t Validation Loss: 0.684959343942513\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 45 \t\t Training Loss: 0.6430127617395922 \t\t Validation Loss: 0.6816848606702341\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 46 \t\t Training Loss: 0.6412283658954915 \t\t Validation Loss: 0.6846937836403866\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 47 \t\t Training Loss: 0.6406696415021454 \t\t Validation Loss: 0.6907175798340147\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 48 \t\t Training Loss: 0.638282302738979 \t\t Validation Loss: 0.6818786258716507\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 49 \t\t Training Loss: 0.637604359218807 \t\t Validation Loss: 0.6779046668949356\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 50 \t\t Training Loss: 0.6366024909951172 \t\t Validation Loss: 0.6786501554853888\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 51 \t\t Training Loss: 0.6350403345550221 \t\t Validation Loss: 0.6797439325378236\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 52 \t\t Training Loss: 0.63317857905199 \t\t Validation Loss: 0.6755994154637553\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 53 \t\t Training Loss: 0.6329022167422996 \t\t Validation Loss: 0.6808440877621867\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 54 \t\t Training Loss: 0.6328519240622505 \t\t Validation Loss: 0.6789492798516474\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 \t\t Training Loss: 0.6305479867087564 \t\t Validation Loss: 0.6765009801226308\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 56 \t\t Training Loss: 0.6297204129242717 \t\t Validation Loss: 0.6777838288075421\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 57 \t\t Training Loss: 0.6284439208685005 \t\t Validation Loss: 0.6770082579190988\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 58 \t\t Training Loss: 0.6283782139044184 \t\t Validation Loss: 0.675219465774369\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 59 \t\t Training Loss: 0.627055182729509 \t\t Validation Loss: 0.6731568707887869\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 60 \t\t Training Loss: 0.6255903462419269 \t\t Validation Loss: 0.672219385426358\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 61 \t\t Training Loss: 0.6248421853867896 \t\t Validation Loss: 0.6766640669796097\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 62 \t\t Training Loss: 0.6238322460022868 \t\t Validation Loss: 0.6775412504891475\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 63 \t\t Training Loss: 0.6229269043977778 \t\t Validation Loss: 0.6703473626379948\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 64 \t\t Training Loss: 0.6222149431177298 \t\t Validation Loss: 0.6747910413609083\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 65 \t\t Training Loss: 0.621872266547364 \t\t Validation Loss: 0.6727996898362361\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 66 \t\t Training Loss: 0.6213467913123956 \t\t Validation Loss: 0.6684704449072302\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 67 \t\t Training Loss: 0.6192152261153016 \t\t Validation Loss: 0.6669916857286279\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 68 \t\t Training Loss: 0.6194190413723701 \t\t Validation Loss: 0.6708754277324297\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 69 \t\t Training Loss: 0.6178481125123848 \t\t Validation Loss: 0.6662066911321237\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 70 \t\t Training Loss: 0.618230052592747 \t\t Validation Loss: 0.6767860670013731\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 71 \t\t Training Loss: 0.6177661999644143 \t\t Validation Loss: 0.6684910557659499\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 72 \t\t Training Loss: 0.6160282830142003 \t\t Validation Loss: 0.6647300829450448\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 73 \t\t Training Loss: 0.616249717808319 \t\t Validation Loss: 0.6692983904682782\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 74 \t\t Training Loss: 0.6142766228623237 \t\t Validation Loss: 0.6666594753227386\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 75 \t\t Training Loss: 0.614065138769192 \t\t Validation Loss: 0.6665229946968565\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 76 \t\t Training Loss: 0.6139593051105249 \t\t Validation Loss: 0.667592533318645\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 77 \t\t Training Loss: 0.6126744524799832 \t\t Validation Loss: 0.6658400871839182\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 78 \t\t Training Loss: 0.6121669087491313 \t\t Validation Loss: 0.6632351573719921\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 79 \t\t Training Loss: 0.6118334181075292 \t\t Validation Loss: 0.6655715397629605\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 80 \t\t Training Loss: 0.6109575798646718 \t\t Validation Loss: 0.6632456496892223\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 81 \t\t Training Loss: 0.610114959195276 \t\t Validation Loss: 0.6650307919399672\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 82 \t\t Training Loss: 0.6102064841079881 \t\t Validation Loss: 0.6660820319358096\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 83 \t\t Training Loss: 0.6089319710743126 \t\t Validation Loss: 0.6655884822051363\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 84 \t\t Training Loss: 0.6084642999194394 \t\t Validation Loss: 0.6648897853980501\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 85 \t\t Training Loss: 0.6081684404207534 \t\t Validation Loss: 0.6620914767462894\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 86 \t\t Training Loss: 0.6074103279535253 \t\t Validation Loss: 0.664077144932462\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 87 \t\t Training Loss: 0.6070579395629053 \t\t Validation Loss: 0.662251854322821\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 88 \t\t Training Loss: 0.6063623432438089 \t\t Validation Loss: 0.6610897848330646\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 89 \t\t Training Loss: 0.6059617274727608 \t\t Validation Loss: 0.6620581795969808\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 90 \t\t Training Loss: 0.6057995994982595 \t\t Validation Loss: 0.6612780729375513\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 91 \t\t Training Loss: 0.6043479124634573 \t\t Validation Loss: 0.658948891666306\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 92 \t\t Training Loss: 0.6044874500267052 \t\t Validation Loss: 0.6606381039220498\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 93 \t\t Training Loss: 0.6034493857006822 \t\t Validation Loss: 0.6608604556060882\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 94 \t\t Training Loss: 0.6033427581937111 \t\t Validation Loss: 0.6598660102878434\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 95 \t\t Training Loss: 0.6025698182062074 \t\t Validation Loss: 0.6592823373843949\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 96 \t\t Training Loss: 0.6022882844079211 \t\t Validation Loss: 0.6586362555682421\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 97 \t\t Training Loss: 0.6016626373349326 \t\t Validation Loss: 0.6615486453728847\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 98 \t\t Training Loss: 0.6016478192029825 \t\t Validation Loss: 0.6570976414528501\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 99 \t\t Training Loss: 0.6009871993753622 \t\t Validation Loss: 0.6549656873205268\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 100 \t\t Training Loss: 0.6004276285670107 \t\t Validation Loss: 0.6600092224390858\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 101 \t\t Training Loss: 0.6002369963943193 \t\t Validation Loss: 0.6584609207879024\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 102 \t\t Training Loss: 0.5990168172857854 \t\t Validation Loss: 0.6577021534224431\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 103 \t\t Training Loss: 0.5990184261094272 \t\t Validation Loss: 0.661456221840771\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 104 \t\t Training Loss: 0.5984621285652766 \t\t Validation Loss: 0.6584594453948428\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 105 \t\t Training Loss: 0.5983166577914251 \t\t Validation Loss: 0.654836865772764\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 106 \t\t Training Loss: 0.597870838235425 \t\t Validation Loss: 0.6585565786437685\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 107 \t\t Training Loss: 0.5976282149672139 \t\t Validation Loss: 0.6550391029076748\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 108 \t\t Training Loss: 0.5971655245238191 \t\t Validation Loss: 0.6597855454422088\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 109 \t\t Training Loss: 0.5964110096603777 \t\t Validation Loss: 0.6530652388158548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 110 \t\t Training Loss: 0.5959522685594775 \t\t Validation Loss: 0.6526632772023935\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 111 \t\t Training Loss: 0.595579131137073 \t\t Validation Loss: 0.6552325059693173\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 112 \t\t Training Loss: 0.5954331194319237 \t\t Validation Loss: 0.6542397668637128\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 113 \t\t Training Loss: 0.5948071575133105 \t\t Validation Loss: 0.6537913729945027\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 114 \t\t Training Loss: 0.5938852052668528 \t\t Validation Loss: 0.6530044164315638\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 115 \t\t Training Loss: 0.5943718254434148 \t\t Validation Loss: 0.6534313060847886\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 116 \t\t Training Loss: 0.5934083376857207 \t\t Validation Loss: 0.6529098936761043\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 117 \t\t Training Loss: 0.5941330867765433 \t\t Validation Loss: 0.6512164604141417\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 118 \t\t Training Loss: 0.5929766039590578 \t\t Validation Loss: 0.649966537240017\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 119 \t\t Training Loss: 0.5922714890264335 \t\t Validation Loss: 0.6474674569658074\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 120 \t\t Training Loss: 0.5924354188068282 \t\t Validation Loss: 0.6527560077815416\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 121 \t\t Training Loss: 0.5917322352152247 \t\t Validation Loss: 0.6517143508352606\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 122 \t\t Training Loss: 0.5916626388830161 \t\t Validation Loss: 0.65276882064295\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 123 \t\t Training Loss: 0.5913540692682229 \t\t Validation Loss: 0.6521483607501147\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 124 \t\t Training Loss: 0.5907453799807608 \t\t Validation Loss: 0.6535741511094143\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 125 \t\t Training Loss: 0.5903565771663194 \t\t Validation Loss: 0.6533817337803631\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 126 \t\t Training Loss: 0.5901328227294986 \t\t Validation Loss: 0.6510588711951358\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 127 \t\t Training Loss: 0.5901776731648063 \t\t Validation Loss: 0.6514878356124301\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 128 \t\t Training Loss: 0.589295015353041 \t\t Validation Loss: 0.6523536137375698\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 129 \t\t Training Loss: 0.5893986649121338 \t\t Validation Loss: 0.6513889040130069\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 130 \t\t Training Loss: 0.5889940700490864 \t\t Validation Loss: 0.6499086103116374\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 131 \t\t Training Loss: 0.5886497244298273 \t\t Validation Loss: 0.6501772246987696\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 132 \t\t Training Loss: 0.5878426215793371 \t\t Validation Loss: 0.650029108581315\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 133 \t\t Training Loss: 0.5882437596898931 \t\t Validation Loss: 0.6504219210005376\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 134 \t\t Training Loss: 0.588463200263656 \t\t Validation Loss: 0.6497517479843352\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 135 \t\t Training Loss: 0.5871613004142118 \t\t Validation Loss: 0.6491364076792956\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 136 \t\t Training Loss: 0.5873281280974345 \t\t Validation Loss: 0.6500805459649439\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 137 \t\t Training Loss: 0.5870129850568844 \t\t Validation Loss: 0.6485692336264834\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 138 \t\t Training Loss: 0.5868022735925918 \t\t Validation Loss: 0.647322559261702\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 139 \t\t Training Loss: 0.5868109497392289 \t\t Validation Loss: 0.6465100949978924\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 140 \t\t Training Loss: 0.5862302741066354 \t\t Validation Loss: 0.6476576181046991\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 141 \t\t Training Loss: 0.5857669917365645 \t\t Validation Loss: 0.6475251001190854\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 142 \t\t Training Loss: 0.585817065540541 \t\t Validation Loss: 0.6424879488717037\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 143 \t\t Training Loss: 0.5854525709896949 \t\t Validation Loss: 0.649277067041967\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 144 \t\t Training Loss: 0.5851637697293793 \t\t Validation Loss: 0.6449467508916361\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 145 \t\t Training Loss: 0.5853937471208712 \t\t Validation Loss: 0.650231627354109\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 146 \t\t Training Loss: 0.5845978527693395 \t\t Validation Loss: 0.6479942734497952\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 147 \t\t Training Loss: 0.5844475384512781 \t\t Validation Loss: 0.6464171032031694\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 148 \t\t Training Loss: 0.5843775123687883 \t\t Validation Loss: 0.6463879114128204\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 149 \t\t Training Loss: 0.584079615022421 \t\t Validation Loss: 0.6495097353163943\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 150 \t\t Training Loss: 0.5839467159165816 \t\t Validation Loss: 0.6510546698988197\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 151 \t\t Training Loss: 0.5838455207642592 \t\t Validation Loss: 0.6461008753909533\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 152 \t\t Training Loss: 0.5830417127011358 \t\t Validation Loss: 0.6456368259224758\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 153 \t\t Training Loss: 0.5831248649174416 \t\t Validation Loss: 0.6456749926050346\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 154 \t\t Training Loss: 0.5825689936640203 \t\t Validation Loss: 0.6428522851125178\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 155 \t\t Training Loss: 0.5829021896495193 \t\t Validation Loss: 0.6466064153914433\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 156 \t\t Training Loss: 0.5821393453208651 \t\t Validation Loss: 0.6462664865402586\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 157 \t\t Training Loss: 0.5816509891227118 \t\t Validation Loss: 0.6469637613847437\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 158 \t\t Training Loss: 0.5817958848451057 \t\t Validation Loss: 0.6445446505964515\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 159 \t\t Training Loss: 0.5813043011623935 \t\t Validation Loss: 0.6442797620932894\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 160 \t\t Training Loss: 0.5821325091408268 \t\t Validation Loss: 0.6461614783541615\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 161 \t\t Training Loss: 0.5804812345929414 \t\t Validation Loss: 0.6444353710607703\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 162 \t\t Training Loss: 0.5808633703756607 \t\t Validation Loss: 0.6435666250517644\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 163 \t\t Training Loss: 0.5814804375356523 \t\t Validation Loss: 0.644638216115564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 164 \t\t Training Loss: 0.5801159937824356 \t\t Validation Loss: 0.6431497473165808\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 165 \t\t Training Loss: 0.5798867264573533 \t\t Validation Loss: 0.6437022897351785\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 166 \t\t Training Loss: 0.580049793409678 \t\t Validation Loss: 0.6441137102020689\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 167 \t\t Training Loss: 0.5797355563095516 \t\t Validation Loss: 0.6478765540388951\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 168 \t\t Training Loss: 0.5799113126998989 \t\t Validation Loss: 0.6416987023980494\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 169 \t\t Training Loss: 0.5791419286959246 \t\t Validation Loss: 0.6426783163708994\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 170 \t\t Training Loss: 0.5791630895728287 \t\t Validation Loss: 0.6426365199791958\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 171 \t\t Training Loss: 0.5788425758025474 \t\t Validation Loss: 0.6442275526989029\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 172 \t\t Training Loss: 0.5786538241999731 \t\t Validation Loss: 0.6444178183240244\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 173 \t\t Training Loss: 0.5783813961137434 \t\t Validation Loss: 0.645921568471597\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 174 \t\t Training Loss: 0.5787367440717222 \t\t Validation Loss: 0.6426276277260952\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 175 \t\t Training Loss: 0.578257783937095 \t\t Validation Loss: 0.6435794447997651\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 176 \t\t Training Loss: 0.5781526467443093 \t\t Validation Loss: 0.6481040300601032\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 177 \t\t Training Loss: 0.577524995957004 \t\t Validation Loss: 0.6414585894797428\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 178 \t\t Training Loss: 0.5773139378376962 \t\t Validation Loss: 0.6446388204734164\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 179 \t\t Training Loss: 0.5773463519632579 \t\t Validation Loss: 0.6436588835431285\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 180 \t\t Training Loss: 0.5768553327288731 \t\t Validation Loss: 0.6435946579948365\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 181 \t\t Training Loss: 0.5776124245094736 \t\t Validation Loss: 0.6433832799770918\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 182 \t\t Training Loss: 0.5767187417926755 \t\t Validation Loss: 0.6442857384681702\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 183 \t\t Training Loss: 0.5764427622916374 \t\t Validation Loss: 0.6435850060793508\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 184 \t\t Training Loss: 0.5764910216135001 \t\t Validation Loss: 0.645273199831822\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 185 \t\t Training Loss: 0.5761283136042653 \t\t Validation Loss: 0.6401700199362766\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 186 \t\t Training Loss: 0.5761238344617792 \t\t Validation Loss: 0.6421901885256824\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 187 \t\t Training Loss: 0.5755164245350677 \t\t Validation Loss: 0.6433172499041159\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 188 \t\t Training Loss: 0.5756582179505793 \t\t Validation Loss: 0.642831861022934\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 189 \t\t Training Loss: 0.5752213012871089 \t\t Validation Loss: 0.642612842449629\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 190 \t\t Training Loss: 0.5754945559289322 \t\t Validation Loss: 0.6448824180074897\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 191 \t\t Training Loss: 0.5747427132655732 \t\t Validation Loss: 0.6416568350032031\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 192 \t\t Training Loss: 0.5749650803560908 \t\t Validation Loss: 0.6415743250770872\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 193 \t\t Training Loss: 0.5747976485321254 \t\t Validation Loss: 0.6436487944477582\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 194 \t\t Training Loss: 0.5745725970538675 \t\t Validation Loss: 0.6424998664760969\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 195 \t\t Training Loss: 0.5744837729815735 \t\t Validation Loss: 0.6383176137251683\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 196 \t\t Training Loss: 0.5744197878707654 \t\t Validation Loss: 0.6415952049878489\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 197 \t\t Training Loss: 0.5745335113410671 \t\t Validation Loss: 0.6415794209180126\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 198 \t\t Training Loss: 0.573922480782102 \t\t Validation Loss: 0.6427153194568072\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 199 \t\t Training Loss: 0.5735180573649358 \t\t Validation Loss: 0.6438605528903673\n",
      "Wrong train batch size. Skipping batch.\n",
      "Throwing away 13 samples.\n",
      "Epoch 200 \t\t Training Loss: 0.5737606484276244 \t\t Validation Loss: 0.6403487013630658\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Set device for model\n",
    "net = net.to(device)\n",
    "\n",
    "# Select optimizerand loss criteria\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "# Training the model\n",
    "for epoch in range(200):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training\n",
    "    net.train()\n",
    "    for local_batch, local_labels in dataloader_train:\n",
    "        if local_batch.shape[0] != batch_size:\n",
    "            print(f\"Wrong train batch size. Skipping batch.\\nThrowing away {local_batch.shape[0]} samples.\")\n",
    "            continue\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model \n",
    "        y_pred = net(local_batch)\n",
    "        # Compute and print loss \n",
    "        loss = criterion(y_pred, local_labels)\n",
    "        # Zero gradients, perform a backward pass, update the weights. \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        # Update loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    valid_loss = 0.0\n",
    "    for data, labels in dataloader_test:\n",
    "        if data.shape[0] != batch_size:\n",
    "            continue\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        target = net(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(dataloader_train)} \\t\\t Validation Loss: {valid_loss / len(dataloader_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653123f",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1efc51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport backtest as bt\\nfrom strategy import PretrainedModelStrategy, SignalModelStrategy\\nfrom technical_signals import TechnicalSignalSet\\nfrom sklearn.svm import SVR\\n\\nrandom.shuffle(spy_constituents)\\n\\n# XXX temporary - need to rework concurrency to be suitable for CUDA\\n# (Must use `spawn` as opposed to `fork` based concurrency I believe - separate OS processes?)\\nnet = net.to(torch.device(\\'cpu\\'))\\n\\ndef predict(net):\\n    return lambda X:        net(torch.from_numpy(X).float().cpu()).detach().numpy()\\n\\ndef df_to_signal_set(df):\\n    return TechnicalSignalSet(df, predict_window=predict_window)\\n\\nstrategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=0.95, bias=0.2)\\n#strategy = SignalModelStrategy(SVR(), lambda df: TechnicalSignalSet(df, predict_window=14), cutoff=1., bias=0.1)\\nbt.comprehensive_backtest(strategy, spy_constituents[:1], \"2000-01-01\", \"2025-01-01\", plot=True, train_test_ratio=0.8)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import backtest as bt\n",
    "from strategy import PretrainedModelStrategy, SignalModelStrategy\n",
    "from technical_signals import TechnicalSignalSet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "random.shuffle(spy_constituents)\n",
    "\n",
    "# XXX temporary - need to rework concurrency to be suitable for CUDA\n",
    "# (Must use `spawn` as opposed to `fork` based concurrency I believe - separate OS processes?)\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def predict(net):\n",
    "    return lambda X:\\\n",
    "        net(torch.from_numpy(X).float().cpu()).detach().numpy()\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "strategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=0.95, bias=0.2)\n",
    "#strategy = SignalModelStrategy(SVR(), lambda df: TechnicalSignalSet(df, predict_window=14), cutoff=1., bias=0.1)\n",
    "bt.comprehensive_backtest(strategy, spy_constituents[:1], \"2000-01-01\", \"2025-01-01\", plot=True, train_test_ratio=0.8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba0315",
   "metadata": {},
   "source": [
    "## Generate and Store Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba68d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction import Prediction\n",
    "from predict import predict_price_change\n",
    "from predictive_model import PredictiveModel\n",
    "from datetime import datetime\n",
    "\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "model = PredictiveModel(net, \"TorchMATI\", predict_window, datetime.now())\n",
    "\n",
    "predictions = predict_price_change(model, df_to_signal_set, tickers[:10])\n",
    "ds.save_predictions([p for t, p in predictions.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f1dd2",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f547e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f160a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
