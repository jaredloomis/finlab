{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2766d46",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74935c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Suppress ta warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Auto reload local files\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Make files in src/ available to notebook\n",
    "import sys\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1408a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SPY csv, define config\n",
    "spy_constituents = list(pd.read_csv('../../data/spy_constituents.csv', header=0)['Symbol'])\n",
    "random.shuffle(spy_constituents)\n",
    "\n",
    "tickers = spy_constituents\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "predict_window = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7fb3d",
   "metadata": {},
   "source": [
    "## Sync & Load Data, Create Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e44208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading daily candlesticks for VIAC:\n",
      "No data fetched for symbol VIAC using YahooDailyReader\n",
      "Error downloading daily candlesticks for PBCT:\n",
      "No data fetched for symbol PBCT using YahooDailyReader\n",
      "Error downloading daily candlesticks for BLL:\n",
      "No data fetched for symbol BLL using YahooDailyReader\n",
      "Error downloading daily candlesticks for CERN:\n",
      "No data fetched for symbol CERN using YahooDailyReader\n",
      "Error downloading daily candlesticks for BRK.B:\n",
      "'Date'\n",
      "Error downloading daily candlesticks for DISCK:\n",
      "No data fetched for symbol DISCK using YahooDailyReader\n",
      "Error downloading daily candlesticks for INFO:\n",
      "No data fetched for symbol INFO using YahooDailyReader\n",
      "Error downloading daily candlesticks for XLNX:\n",
      "No data fetched for symbol XLNX using YahooDailyReader\n",
      "Error downloading daily candlesticks for BF.B:\n",
      "'Date'\n",
      "Error downloading daily candlesticks for WLTW:\n",
      "No data fetched for symbol WLTW using YahooDailyReader\n",
      "Error downloading daily candlesticks for KSU:\n",
      "No data fetched for symbol KSU using YahooDailyReader\n",
      "Error downloading daily candlesticks for DISCA:\n",
      "No data fetched for symbol DISCA using YahooDailyReader\n",
      "Exception on VIAC:\n",
      "'high'\n",
      "Exception on PBCT:\n",
      "'high'\n",
      "Exception on BRK.B:\n",
      "'high'\n",
      "Exception on DISCK:\n",
      "'high'\n",
      "Exception on INFO:\n",
      "'high'\n",
      "Exception on XLNX:\n",
      "'high'\n",
      "Exception on BF.B:\n",
      "'high'\n",
      "Exception on WLTW:\n",
      "'high'\n",
      "Exception on OGN:\n",
      "Found array with 0 sample(s) (shape=(0, 58)) while a minimum of 1 is required by StandardScaler.\n",
      "Exception on KSU:\n",
      "'high'\n",
      "Exception on DISCA:\n",
      "'high'\n"
     ]
    }
   ],
   "source": [
    "# Load the data from db\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datastore as ds\n",
    "from technical_signals import TechnicalSignalSet\n",
    "\n",
    "ds.download_daily_candlesticks(tickers, start_date, end_date)\n",
    "candlesticks = ds.get_daily_candlesticks(tickers, start_date, end_date)\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        technical_sigs = TechnicalSignalSet(candlesticks[ticker], predict_window)\n",
    "        X, y, Xy_date = technical_sigs.to_xy()\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "    except Exception as ex:\n",
    "        print(f\"Exception on {ticker}:\")\n",
    "        print(ex)\n",
    "\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "y = np.concatenate(ys, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b9a72",
   "metadata": {},
   "source": [
    "## Clean Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c319093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2059299, 58])\n",
      "Batch size: 1023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "\n",
    "\n",
    "def round_batch_size(sample_count, approximately, leeway=None):\n",
    "    \"\"\"\n",
    "    Round batch size to a more suitable value. This helps to avoid a\n",
    "    problem where the final batch has a lot of samples, but not enough for\n",
    "    a full batch, leading to many samples being thrown out.\n",
    "\n",
    "    approximately: int, leeway: int\n",
    "      decide on a chunk size around a number, with specified leeway\n",
    "      (leeway defaults to `approximately // 10`).\n",
    "    \"\"\"\n",
    "    if leeway is None:\n",
    "        leeway = approximately // 10\n",
    "    \n",
    "    # Get the number of leftover samples if we use the suggested batch size\n",
    "    best_leftover = sample_count - np.floor(sample_count / approximately) * approximately\n",
    "\n",
    "    # Brute-force search for the value that yeilds the fewest leftovers\n",
    "    # within the given leeway range.\n",
    "    best_chunk_count = approximately\n",
    "    for offset in range(-leeway, leeway):\n",
    "        chunk_size = approximately + offset\n",
    "        leftover = sample_count - np.floor(sample_count / chunk_size) * chunk_size\n",
    "        if leftover < best_leftover:\n",
    "            best_leftover = leftover\n",
    "            best_chunk_count = chunk_size\n",
    "    return best_chunk_count\n",
    "            \n",
    "\n",
    "batch_size = round_batch_size(X_train.shape[0], 1024, leeway=200)\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Convert X, y to torch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.reshape(y_train.shape[0], 1)).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.reshape(y_test.shape[0], 1)).float()\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print('Batch size:', batch_size)\n",
    "\n",
    "# Generators\n",
    "training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader_train = DataLoader(training_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "validation_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "dataloader_test = DataLoader(validation_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Release duplicated memory\n",
    "try:\n",
    "    del X\n",
    "    del y\n",
    "    del Xs\n",
    "    del ys\n",
    "    #del X_train\n",
    "    #del X_test\n",
    "    #del y_train\n",
    "    #del y_test\n",
    "    del X_train_tensor\n",
    "    del X_test_tensor\n",
    "    del y_train_tensor\n",
    "    del y_test_tensor\n",
    "except:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97b800",
   "metadata": {},
   "source": [
    "## Create the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35154ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 1\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, n_outputs),\n",
    ")\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, n_outputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374307cb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce603e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.9310950853311772 \t\t Validation Loss: 0.8938240488725049\n",
      "Epoch 2 \t\t Training Loss: 0.8858433501731088 \t\t Validation Loss: 0.8692614970994847\n",
      "Epoch 3 \t\t Training Loss: 0.8629090928107545 \t\t Validation Loss: 0.8515444243592876\n",
      "Epoch 4 \t\t Training Loss: 0.8449403086054461 \t\t Validation Loss: 0.8386445564350912\n",
      "Epoch 5 \t\t Training Loss: 0.8309071763676313 \t\t Validation Loss: 0.8224686865827867\n",
      "Epoch 6 \t\t Training Loss: 0.8181151853114648 \t\t Validation Loss: 0.8128670432737896\n",
      "Epoch 7 \t\t Training Loss: 0.8061985162082621 \t\t Validation Loss: 0.8043306044169835\n",
      "Epoch 8 \t\t Training Loss: 0.7956835946454022 \t\t Validation Loss: 0.7997041254171303\n",
      "Epoch 9 \t\t Training Loss: 0.7859410556820691 \t\t Validation Loss: 0.7916998474725655\n",
      "Epoch 10 \t\t Training Loss: 0.7784018351317518 \t\t Validation Loss: 0.7882710402565343\n",
      "Epoch 11 \t\t Training Loss: 0.7715815457312314 \t\t Validation Loss: 0.7816575454281909\n",
      "Epoch 12 \t\t Training Loss: 0.7643593970780579 \t\t Validation Loss: 0.7766890211829117\n",
      "Epoch 13 \t\t Training Loss: 0.7578765627346522 \t\t Validation Loss: 0.7735205463000706\n",
      "Epoch 14 \t\t Training Loss: 0.7528176263559534 \t\t Validation Loss: 0.767709626683167\n",
      "Epoch 15 \t\t Training Loss: 0.7471824294586296 \t\t Validation Loss: 0.7662076391279697\n",
      "Epoch 16 \t\t Training Loss: 0.7417512645605365 \t\t Validation Loss: 0.7576028313487768\n",
      "Epoch 17 \t\t Training Loss: 0.7377333915239535 \t\t Validation Loss: 0.7591462023556232\n",
      "Epoch 18 \t\t Training Loss: 0.7341898876934932 \t\t Validation Loss: 0.7521884970899139\n",
      "Epoch 19 \t\t Training Loss: 0.7297276545980352 \t\t Validation Loss: 0.7558028530329466\n",
      "Epoch 20 \t\t Training Loss: 0.7258804722088105 \t\t Validation Loss: 0.7514873048556703\n",
      "Epoch 21 \t\t Training Loss: 0.722418532848832 \t\t Validation Loss: 0.7463267062391553\n",
      "Epoch 22 \t\t Training Loss: 0.7193724052916696 \t\t Validation Loss: 0.7449936576719794\n",
      "Epoch 23 \t\t Training Loss: 0.7158386534384336 \t\t Validation Loss: 0.7440030042614255\n",
      "Epoch 24 \t\t Training Loss: 0.7134175765176821 \t\t Validation Loss: 0.7447808435452836\n",
      "Epoch 25 \t\t Training Loss: 0.7094815318938577 \t\t Validation Loss: 0.7381818704307079\n",
      "Epoch 26 \t\t Training Loss: 0.7074948895409874 \t\t Validation Loss: 0.7370432947895357\n",
      "Epoch 27 \t\t Training Loss: 0.7043334305197578 \t\t Validation Loss: 0.739113394969276\n",
      "Epoch 28 \t\t Training Loss: 0.702728421961675 \t\t Validation Loss: 0.7355935334094933\n",
      "Epoch 29 \t\t Training Loss: 0.699661022800743 \t\t Validation Loss: 0.7307592265840087\n",
      "Epoch 30 \t\t Training Loss: 0.6983408991874774 \t\t Validation Loss: 0.73122195340693\n",
      "Epoch 31 \t\t Training Loss: 0.6956792763969641 \t\t Validation Loss: 0.7273981009743044\n",
      "Epoch 32 \t\t Training Loss: 0.6932762240333102 \t\t Validation Loss: 0.7265328671783209\n",
      "Epoch 33 \t\t Training Loss: 0.6914869744980211 \t\t Validation Loss: 0.7266027594783476\n",
      "Epoch 34 \t\t Training Loss: 0.6897540316910987 \t\t Validation Loss: 0.7261835775737252\n",
      "Epoch 35 \t\t Training Loss: 0.6876598542090737 \t\t Validation Loss: 0.7213170166526522\n",
      "Epoch 36 \t\t Training Loss: 0.6862602379863321 \t\t Validation Loss: 0.7219335955700704\n",
      "Epoch 37 \t\t Training Loss: 0.684235245655617 \t\t Validation Loss: 0.717097506193178\n",
      "Epoch 38 \t\t Training Loss: 0.6821405758208678 \t\t Validation Loss: 0.7198712235050542\n",
      "Epoch 39 \t\t Training Loss: 0.680695034382181 \t\t Validation Loss: 0.7170314065047673\n",
      "Epoch 40 \t\t Training Loss: 0.6792135363008865 \t\t Validation Loss: 0.7156403809785843\n",
      "Epoch 41 \t\t Training Loss: 0.677657989619445 \t\t Validation Loss: 0.7185849286615849\n",
      "Epoch 42 \t\t Training Loss: 0.6765492458397988 \t\t Validation Loss: 0.7171557614845889\n",
      "Epoch 43 \t\t Training Loss: 0.6748918747049153 \t\t Validation Loss: 0.7162067123821804\n",
      "Epoch 44 \t\t Training Loss: 0.6730518371562607 \t\t Validation Loss: 0.7114323287137917\n",
      "Epoch 45 \t\t Training Loss: 0.6716190525624388 \t\t Validation Loss: 0.7097260339983872\n",
      "Epoch 46 \t\t Training Loss: 0.6706203827500284 \t\t Validation Loss: 0.7151928490826062\n",
      "Epoch 47 \t\t Training Loss: 0.6699297932683089 \t\t Validation Loss: 0.7123334053903818\n",
      "Epoch 48 \t\t Training Loss: 0.6684205905454288 \t\t Validation Loss: 0.7090080595974412\n",
      "Epoch 49 \t\t Training Loss: 0.6673424040507275 \t\t Validation Loss: 0.7050239592790604\n",
      "Epoch 50 \t\t Training Loss: 0.6658171013523202 \t\t Validation Loss: 0.7055363817406552\n",
      "Epoch 51 \t\t Training Loss: 0.665344302653082 \t\t Validation Loss: 0.7083383218518325\n",
      "Epoch 52 \t\t Training Loss: 0.6633048405768086 \t\t Validation Loss: 0.7061615985419069\n",
      "Epoch 53 \t\t Training Loss: 0.6629175707231513 \t\t Validation Loss: 0.7041960097849369\n",
      "Epoch 54 \t\t Training Loss: 0.6616932800381088 \t\t Validation Loss: 0.7033173556306532\n",
      "Epoch 55 \t\t Training Loss: 0.6605409580679361 \t\t Validation Loss: 0.7054724632097142\n",
      "Epoch 56 \t\t Training Loss: 0.659969400542326 \t\t Validation Loss: 0.7026830147951841\n",
      "Epoch 57 \t\t Training Loss: 0.6586007541586851 \t\t Validation Loss: 0.7027731032243797\n",
      "Epoch 58 \t\t Training Loss: 0.6580704900381048 \t\t Validation Loss: 0.7018610898937497\n",
      "Epoch 59 \t\t Training Loss: 0.6571085628916006 \t\t Validation Loss: 0.6998926087149552\n",
      "Epoch 60 \t\t Training Loss: 0.6559693775423356 \t\t Validation Loss: 0.7023864180913993\n",
      "Epoch 61 \t\t Training Loss: 0.6546434961736824 \t\t Validation Loss: 0.7041398065962962\n",
      "Epoch 62 \t\t Training Loss: 0.6543547022242009 \t\t Validation Loss: 0.7016302089073828\n",
      "Epoch 63 \t\t Training Loss: 0.6531958667958369 \t\t Validation Loss: 0.6994748381631715\n",
      "Epoch 64 \t\t Training Loss: 0.6522325568985595 \t\t Validation Loss: 0.6970575124557529\n",
      "Epoch 65 \t\t Training Loss: 0.6515051877090011 \t\t Validation Loss: 0.6972716426742929\n",
      "Epoch 66 \t\t Training Loss: 0.650916722895077 \t\t Validation Loss: 0.6947950619672026\n",
      "Epoch 67 \t\t Training Loss: 0.6497632382938179 \t\t Validation Loss: 0.695364473387599\n",
      "Epoch 68 \t\t Training Loss: 0.6494128516344543 \t\t Validation Loss: 0.7018251913998809\n",
      "Epoch 69 \t\t Training Loss: 0.6489369423899909 \t\t Validation Loss: 0.6966751643589565\n",
      "Epoch 70 \t\t Training Loss: 0.6473832297135631 \t\t Validation Loss: 0.6928915032850844\n",
      "Epoch 71 \t\t Training Loss: 0.6472646161447516 \t\t Validation Loss: 0.697165025132043\n",
      "Epoch 72 \t\t Training Loss: 0.6463147223262247 \t\t Validation Loss: 0.6977661903947592\n",
      "Epoch 73 \t\t Training Loss: 0.6452793627015051 \t\t Validation Loss: 0.6947976874985865\n",
      "Epoch 74 \t\t Training Loss: 0.645053511937933 \t\t Validation Loss: 0.6964679437556437\n",
      "Epoch 75 \t\t Training Loss: 0.6440015967307965 \t\t Validation Loss: 0.6971366557159594\n",
      "Epoch 76 \t\t Training Loss: 0.6436377584608526 \t\t Validation Loss: 0.6929486670664379\n",
      "Epoch 77 \t\t Training Loss: 0.6432094160028791 \t\t Validation Loss: 0.6958615354129246\n",
      "Epoch 78 \t\t Training Loss: 0.642489108794137 \t\t Validation Loss: 0.6945952895496573\n",
      "Epoch 79 \t\t Training Loss: 0.6422205292284341 \t\t Validation Loss: 0.6895563325711659\n",
      "Epoch 80 \t\t Training Loss: 0.6416015781932249 \t\t Validation Loss: 0.6888128085328\n",
      "Epoch 81 \t\t Training Loss: 0.64011635836001 \t\t Validation Loss: 0.6920983120799065\n",
      "Epoch 82 \t\t Training Loss: 0.6398799279644545 \t\t Validation Loss: 0.6914737251188073\n",
      "Epoch 83 \t\t Training Loss: 0.6392431037673534 \t\t Validation Loss: 0.6913915531975883\n",
      "Epoch 84 \t\t Training Loss: 0.6390202373517904 \t\t Validation Loss: 0.6920416568006788\n",
      "Epoch 85 \t\t Training Loss: 0.6386000953144655 \t\t Validation Loss: 0.6944821636591639\n",
      "Epoch 86 \t\t Training Loss: 0.6378352592906962 \t\t Validation Loss: 0.6912096861217704\n",
      "Epoch 87 \t\t Training Loss: 0.636989613226025 \t\t Validation Loss: 0.6914885358086654\n",
      "Epoch 88 \t\t Training Loss: 0.6370068737303387 \t\t Validation Loss: 0.6882248709776572\n",
      "Epoch 89 \t\t Training Loss: 0.6358754615018573 \t\t Validation Loss: 0.6874600271029132\n",
      "Epoch 90 \t\t Training Loss: 0.6354911085449518 \t\t Validation Loss: 0.6893095666808742\n",
      "Epoch 91 \t\t Training Loss: 0.6349790509340483 \t\t Validation Loss: 0.6897320034248489\n",
      "Epoch 92 \t\t Training Loss: 0.6347697524958088 \t\t Validation Loss: 0.6889766452035734\n",
      "Epoch 93 \t\t Training Loss: 0.6341454432902499 \t\t Validation Loss: 0.6905638924134629\n",
      "Epoch 94 \t\t Training Loss: 0.633922410526503 \t\t Validation Loss: 0.6869280529873711\n",
      "Epoch 95 \t\t Training Loss: 0.6328915657833685 \t\t Validation Loss: 0.6897378484053271\n",
      "Epoch 96 \t\t Training Loss: 0.6329531603586716 \t\t Validation Loss: 0.6852429535772119\n",
      "Epoch 97 \t\t Training Loss: 0.6320622303683664 \t\t Validation Loss: 0.6870884589318719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 \t\t Training Loss: 0.631740749237612 \t\t Validation Loss: 0.6843317611409085\n",
      "Epoch 99 \t\t Training Loss: 0.631308998300019 \t\t Validation Loss: 0.6874754979674306\n",
      "Epoch 100 \t\t Training Loss: 0.6306774932178644 \t\t Validation Loss: 0.683356162160635\n",
      "Epoch 101 \t\t Training Loss: 0.6306159685514368 \t\t Validation Loss: 0.6892720112310988\n",
      "Epoch 102 \t\t Training Loss: 0.6299993561620565 \t\t Validation Loss: 0.6880030645323651\n",
      "Epoch 103 \t\t Training Loss: 0.6296885261710935 \t\t Validation Loss: 0.6931235752999783\n",
      "Epoch 104 \t\t Training Loss: 0.6291802481401636 \t\t Validation Loss: 0.6835181768983603\n",
      "Epoch 105 \t\t Training Loss: 0.6287968494201619 \t\t Validation Loss: 0.6816858210201774\n",
      "Epoch 106 \t\t Training Loss: 0.6287958545289227 \t\t Validation Loss: 0.6842825819871255\n",
      "Epoch 107 \t\t Training Loss: 0.6280444303496939 \t\t Validation Loss: 0.6846056796078172\n",
      "Epoch 108 \t\t Training Loss: 0.6278149485588074 \t\t Validation Loss: 0.6831971196723836\n",
      "Epoch 109 \t\t Training Loss: 0.6269932119809628 \t\t Validation Loss: 0.6787801273167133\n",
      "Epoch 110 \t\t Training Loss: 0.6271331462762987 \t\t Validation Loss: 0.6834722461977175\n",
      "Epoch 111 \t\t Training Loss: 0.62636482443196 \t\t Validation Loss: 0.6785913532865899\n",
      "Epoch 112 \t\t Training Loss: 0.6261007631529757 \t\t Validation Loss: 0.6826770600995847\n",
      "Epoch 113 \t\t Training Loss: 0.6256249905283985 \t\t Validation Loss: 0.6872333913509335\n",
      "Epoch 114 \t\t Training Loss: 0.6250859753030246 \t\t Validation Loss: 0.6799337416887283\n",
      "Epoch 115 \t\t Training Loss: 0.6251034007046392 \t\t Validation Loss: 0.6821224412747792\n",
      "Epoch 116 \t\t Training Loss: 0.6244656403622577 \t\t Validation Loss: 0.6780923974833318\n",
      "Epoch 117 \t\t Training Loss: 0.6244631919230742 \t\t Validation Loss: 0.6808017681219748\n",
      "Epoch 118 \t\t Training Loss: 0.6236527881631789 \t\t Validation Loss: 0.6783705132880381\n",
      "Epoch 119 \t\t Training Loss: 0.6232617887109888 \t\t Validation Loss: 0.6805026323667595\n",
      "Epoch 120 \t\t Training Loss: 0.6224946565907569 \t\t Validation Loss: 0.6772502719291619\n",
      "Epoch 121 \t\t Training Loss: 0.6228560927017257 \t\t Validation Loss: 0.6826465164444276\n",
      "Epoch 122 \t\t Training Loss: 0.6225832535210262 \t\t Validation Loss: 0.6783446701509612\n",
      "Epoch 123 \t\t Training Loss: 0.6214879322750795 \t\t Validation Loss: 0.6772809946643454\n",
      "Epoch 124 \t\t Training Loss: 0.621813336889719 \t\t Validation Loss: 0.6802410249199186\n",
      "Epoch 125 \t\t Training Loss: 0.621837185232188 \t\t Validation Loss: 0.6791014506348542\n",
      "Epoch 126 \t\t Training Loss: 0.6210995655686359 \t\t Validation Loss: 0.6764934033687625\n",
      "Epoch 127 \t\t Training Loss: 0.6205067401552935 \t\t Validation Loss: 0.6755925456860236\n",
      "Epoch 128 \t\t Training Loss: 0.6207449995073819 \t\t Validation Loss: 0.6756273478801761\n",
      "Epoch 129 \t\t Training Loss: 0.619922196189977 \t\t Validation Loss: 0.6764439637107509\n",
      "Epoch 130 \t\t Training Loss: 0.6193411550412886 \t\t Validation Loss: 0.6772364409906524\n",
      "Epoch 131 \t\t Training Loss: 0.6194876674923563 \t\t Validation Loss: 0.6777960690004485\n",
      "Epoch 132 \t\t Training Loss: 0.6192266873856178 \t\t Validation Loss: 0.6784231694681304\n",
      "Epoch 133 \t\t Training Loss: 0.6190650087355146 \t\t Validation Loss: 0.6781359400068011\n",
      "Epoch 134 \t\t Training Loss: 0.6193052779011506 \t\t Validation Loss: 0.6737131351338965\n",
      "Epoch 135 \t\t Training Loss: 0.6183632132844787 \t\t Validation Loss: 0.675281907032643\n",
      "Epoch 136 \t\t Training Loss: 0.6181852898370314 \t\t Validation Loss: 0.6771547546876329\n",
      "Epoch 137 \t\t Training Loss: 0.6179956103579298 \t\t Validation Loss: 0.6764772800462586\n",
      "Epoch 138 \t\t Training Loss: 0.617571352964838 \t\t Validation Loss: 0.6782344114035368\n",
      "Epoch 139 \t\t Training Loss: 0.617193127472011 \t\t Validation Loss: 0.6767294005091701\n",
      "Epoch 140 \t\t Training Loss: 0.6175323559227596 \t\t Validation Loss: 0.6723394308771405\n",
      "Epoch 141 \t\t Training Loss: 0.6167816880681416 \t\t Validation Loss: 0.6735338382422924\n",
      "Epoch 142 \t\t Training Loss: 0.6166521416809567 \t\t Validation Loss: 0.6724714240325349\n",
      "Epoch 143 \t\t Training Loss: 0.616697486158751 \t\t Validation Loss: 0.6761399773614747\n",
      "Epoch 144 \t\t Training Loss: 0.6160490508110323 \t\t Validation Loss: 0.6721557141946894\n",
      "Epoch 145 \t\t Training Loss: 0.6156492168134678 \t\t Validation Loss: 0.6789452146206584\n",
      "Epoch 146 \t\t Training Loss: 0.6152741609523383 \t\t Validation Loss: 0.6766797043383121\n",
      "Epoch 147 \t\t Training Loss: 0.6153602234712005 \t\t Validation Loss: 0.6725553819643599\n",
      "Epoch 148 \t\t Training Loss: 0.6154026924468248 \t\t Validation Loss: 0.6763123273849487\n",
      "Epoch 149 \t\t Training Loss: 0.614755086352798 \t\t Validation Loss: 0.6746703051030636\n",
      "Epoch 150 \t\t Training Loss: 0.6145427828100387 \t\t Validation Loss: 0.6784544994256326\n",
      "Epoch 151 \t\t Training Loss: 0.6144463789001129 \t\t Validation Loss: 0.676157485427601\n",
      "Epoch 152 \t\t Training Loss: 0.6138547599700571 \t\t Validation Loss: 0.6775849565331425\n",
      "Epoch 153 \t\t Training Loss: 0.6139147094808522 \t\t Validation Loss: 0.6716504379042557\n",
      "Epoch 154 \t\t Training Loss: 0.6133184858477062 \t\t Validation Loss: 0.6774335290704455\n",
      "Epoch 155 \t\t Training Loss: 0.613461659283882 \t\t Validation Loss: 0.6709772279219968\n",
      "Epoch 156 \t\t Training Loss: 0.6129797794779793 \t\t Validation Loss: 0.6736357592578445\n",
      "Epoch 157 \t\t Training Loss: 0.6130067832542189 \t\t Validation Loss: 0.6750406229070255\n",
      "Epoch 158 \t\t Training Loss: 0.6125208475047299 \t\t Validation Loss: 0.6738122241305453\n",
      "Epoch 159 \t\t Training Loss: 0.6126410367029316 \t\t Validation Loss: 0.6726347152143717\n",
      "Epoch 160 \t\t Training Loss: 0.6121541658148977 \t\t Validation Loss: 0.672173577493855\n",
      "Epoch 161 \t\t Training Loss: 0.6120699237249292 \t\t Validation Loss: 0.6716084224837167\n",
      "Epoch 162 \t\t Training Loss: 0.6120849477078153 \t\t Validation Loss: 0.6760110293648073\n",
      "Epoch 163 \t\t Training Loss: 0.6113160538898436 \t\t Validation Loss: 0.6745219209364482\n",
      "Epoch 164 \t\t Training Loss: 0.6114531345646949 \t\t Validation Loss: 0.6694849008428199\n",
      "Epoch 165 \t\t Training Loss: 0.6112409694776332 \t\t Validation Loss: 0.6719437520951033\n",
      "Epoch 166 \t\t Training Loss: 0.6107328545378028 \t\t Validation Loss: 0.6714816261082888\n",
      "Epoch 167 \t\t Training Loss: 0.6106170017181554 \t\t Validation Loss: 0.6706185316933053\n",
      "Epoch 168 \t\t Training Loss: 0.6104692812588933 \t\t Validation Loss: 0.6698465685227087\n",
      "Epoch 169 \t\t Training Loss: 0.6102301020321417 \t\t Validation Loss: 0.6692637513790812\n",
      "Epoch 170 \t\t Training Loss: 0.6100887475739767 \t\t Validation Loss: 0.6695680828498942\n",
      "Epoch 171 \t\t Training Loss: 0.6099143298745333 \t\t Validation Loss: 0.6681953171002013\n",
      "Epoch 172 \t\t Training Loss: 0.6093646208711719 \t\t Validation Loss: 0.6694320437631437\n",
      "Epoch 173 \t\t Training Loss: 0.6092574479446089 \t\t Validation Loss: 0.6699275597929955\n",
      "Epoch 174 \t\t Training Loss: 0.6092524123884704 \t\t Validation Loss: 0.6687850808458669\n",
      "Epoch 175 \t\t Training Loss: 0.6091664133235346 \t\t Validation Loss: 0.671016912907362\n",
      "Epoch 176 \t\t Training Loss: 0.6088860571029112 \t\t Validation Loss: 0.6693697875099522\n",
      "Epoch 177 \t\t Training Loss: 0.6084416304366599 \t\t Validation Loss: 0.6704355821545634\n",
      "Epoch 178 \t\t Training Loss: 0.6085514223996052 \t\t Validation Loss: 0.667403819039464\n",
      "Epoch 179 \t\t Training Loss: 0.6085411725326136 \t\t Validation Loss: 0.6670398419456822\n",
      "Epoch 180 \t\t Training Loss: 0.6080186148514157 \t\t Validation Loss: 0.6698815114796162\n",
      "Epoch 181 \t\t Training Loss: 0.607985066674683 \t\t Validation Loss: 0.6693361337695803\n",
      "Epoch 182 \t\t Training Loss: 0.6081074120923766 \t\t Validation Loss: 0.6681878412408488\n",
      "Epoch 183 \t\t Training Loss: 0.6077240410723973 \t\t Validation Loss: 0.6684126209999833\n",
      "Epoch 184 \t\t Training Loss: 0.6073697587476106 \t\t Validation Loss: 0.669939369761518\n",
      "Epoch 185 \t\t Training Loss: 0.6072378447117405 \t\t Validation Loss: 0.6670444126107863\n",
      "Epoch 186 \t\t Training Loss: 0.6071004536367446 \t\t Validation Loss: 0.6710830831101963\n",
      "Epoch 187 \t\t Training Loss: 0.6062383010828015 \t\t Validation Loss: 0.6711253449320793\n",
      "Epoch 188 \t\t Training Loss: 0.6066458107699106 \t\t Validation Loss: 0.6673952412924596\n",
      "Epoch 189 \t\t Training Loss: 0.6059425499714018 \t\t Validation Loss: 0.6689509278429406\n",
      "Epoch 190 \t\t Training Loss: 0.6062191533497537 \t\t Validation Loss: 0.6671600961791617\n",
      "Epoch 191 \t\t Training Loss: 0.6060065894475104 \t\t Validation Loss: 0.6706681640020439\n",
      "Epoch 192 \t\t Training Loss: 0.6058078068468148 \t\t Validation Loss: 0.6659220590123108\n",
      "Epoch 193 \t\t Training Loss: 0.6057675751624745 \t\t Validation Loss: 0.6657471914908716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194 \t\t Training Loss: 0.6051259284817752 \t\t Validation Loss: 0.6646934454994542\n",
      "Epoch 195 \t\t Training Loss: 0.605181006667511 \t\t Validation Loss: 0.6669299458818776\n",
      "Epoch 196 \t\t Training Loss: 0.6052908944183478 \t\t Validation Loss: 0.6655975761158126\n",
      "Epoch 197 \t\t Training Loss: 0.6049639917817569 \t\t Validation Loss: 0.6665186794208628\n",
      "Epoch 198 \t\t Training Loss: 0.6049323970414013 \t\t Validation Loss: 0.6686753782310656\n",
      "Epoch 199 \t\t Training Loss: 0.6046203255831041 \t\t Validation Loss: 0.6667208030287709\n",
      "Epoch 200 \t\t Training Loss: 0.6044722487543924 \t\t Validation Loss: 0.6648910867848566\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Set device for model\n",
    "net = net.to(device)\n",
    "\n",
    "# Select optimizerand loss criteria\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "# Training the model\n",
    "for epoch in range(200):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training\n",
    "    net.train()\n",
    "    for local_batch, local_labels in dataloader_train:\n",
    "        if local_batch.shape[0] != batch_size:\n",
    "            print(f\"Wrong train batch size. Skipping batch.\\nThrowing away {local_batch.shape[0]} samples.\")\n",
    "            continue\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model \n",
    "        y_pred = net(local_batch)\n",
    "        # Compute and print loss \n",
    "        loss = criterion(y_pred, local_labels)\n",
    "        # Zero gradients, perform a backward pass, update the weights. \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        # Update loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    valid_loss = 0.0\n",
    "    for data, labels in dataloader_test:\n",
    "        if data.shape[0] != batch_size:\n",
    "            continue\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        target = net(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(dataloader_train)} \\t\\t Validation Loss: {valid_loss / len(dataloader_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887cdb0",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1efc51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport backtest as bt\\nfrom strategy import PretrainedModelStrategy, SignalModelStrategy\\nfrom technical_signals import TechnicalSignalSet\\nfrom sklearn.svm import SVR\\n\\nrandom.shuffle(spy_constituents)\\n\\n# XXX temporary - need to rework concurrency to be suitable for CUDA\\n# (Must use `spawn` as opposed to `fork` based concurrency I believe - separate OS processes?)\\nnet = net.to(torch.device(\\'cpu\\'))\\n\\ndef predict(net):\\n    return lambda X:        net(torch.from_numpy(X).float().cpu()).detach().numpy()\\n\\ndef df_to_signal_set(df):\\n    return TechnicalSignalSet(df, predict_window=predict_window)\\n\\nstrategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=0.95, bias=0.2)\\n#strategy = SignalModelStrategy(SVR(), lambda df: TechnicalSignalSet(df, predict_window=14), cutoff=1., bias=0.1)\\nbt.comprehensive_backtest(strategy, spy_constituents[:1], \"2000-01-01\", \"2025-01-01\", plot=True, train_test_ratio=0.8)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import backtest as bt\n",
    "from strategy import PretrainedModelStrategy, SignalModelStrategy\n",
    "from technical_signals import TechnicalSignalSet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "random.shuffle(spy_constituents)\n",
    "\n",
    "# XXX temporary - need to rework concurrency to be suitable for CUDA\n",
    "# (Must use `spawn` as opposed to `fork` based concurrency I believe - separate OS processes?)\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def predict(net):\n",
    "    return lambda X:\\\n",
    "        net(torch.from_numpy(X).float().cpu()).detach().numpy()\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "strategy = PretrainedModelStrategy(predict(net), df_to_signal_set, cutoff=0.95, bias=0.2)\n",
    "#strategy = SignalModelStrategy(SVR(), lambda df: TechnicalSignalSet(df, predict_window=14), cutoff=1., bias=0.1)\n",
    "bt.comprehensive_backtest(strategy, spy_constituents[:1], \"2000-01-01\", \"2025-01-01\", plot=True, train_test_ratio=0.8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb1cc3",
   "metadata": {},
   "source": [
    "## Generate and Store Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba68d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on VIAC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on PBCT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on BRK.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on DISCK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on INFO\n",
      "Exception on XLNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on BF.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on WLTW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on OGN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 50, in predict_price_change\n",
      "    X, y, _ = signals.to_xy()\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/signals.py\", line 53, in to_xy\n",
      "    X = self.X_scaler.fit_transform(X)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 809, in fit\n",
      "    return self.partial_fit(X, y, sample_weight)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 844, in partial_fit\n",
      "    X = self._validate_data(\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/sklearn/base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 909, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 58)) while a minimum of 1 is required by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on KSU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on DISCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'high'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/prediction.py\", line 49, in predict_price_change\n",
      "    signals = mk_signal_set(candles[ticker])\n",
      "  File \"/tmp/ipykernel_252085/840965209.py\", line 8, in df_to_signal_set\n",
      "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
      "  File \"/home/jared/workspace/market-diff/stock-prediction/notebooks/../src/technical_signals.py\", line 12, in __init__\n",
      "    donchian = ta.volatility.DonchianChannel(data[\"high\"], data[\"low\"], data[\"close\"])\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/jared/.local/share/virtualenvs/stock-prediction-MOS0QyR2/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'high'\n"
     ]
    }
   ],
   "source": [
    "from prediction import Prediction, predict_price_change\n",
    "from predictive_model import PredictiveModel\n",
    "from datetime import datetime\n",
    "\n",
    "net = net.to(torch.device('cpu'))\n",
    "\n",
    "def df_to_signal_set(df):\n",
    "    return TechnicalSignalSet(df, predict_window=predict_window)\n",
    "\n",
    "model = PredictiveModel(net, \"TorchMATI\", predict_window, datetime.now())\n",
    "\n",
    "predictions = predict_price_change(model, df_to_signal_set, tickers)\n",
    "ds.save_predictions([p for t, p in predictions.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67511e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
